---
title: "The Question is the Answer"
subtitle: "Using LLMs as Interviewers"
description: ""
image:
  src: "/images/the-question-is-the-answer-cover.webp"
  alt: "An abstract digital illustration of a man holding a clipboard and a pencil."
  caption: "Generated by DALL-E"
publishDate: "2024-07-31"
tags:
  - "llm"
  - "rag"
  - "documentation"
  - "knowledge-base"
recommended: []
---

# The Question is the Answer: LLMs as Interviewers

It seems like most of the current use cases for LLMs have to do with summarization and RAG. Both involve an LLM answering questions, for a user, about a piece of text. Either "summarize the following..." or "reference the following document...answer the following question...".

By flipping that paradigm -- using LLMs to generate questions for a user to answer -- there would be a huge potential value unlock.


## Setting the Scene

I think we can all agree, good documentation is vital. Maybe you're documenting your code, maybe you're documenting a business process, or maybe you're taking scratch notes to remember what you just did. It's vital for other people on your team. It's vital when you leave your company, for the next person who replaces you. It's even just for you in 3 months, when you forget how something works.

The act of documenting can be helpful, too. The writing process can help you solidify your thought process and can help commit things to memory.

But, all that being said, good documentation takes time -- time to make, time to organize, and time to maintain. Sometimes it's worth it and sometimes it may not be. And what if we could use LLMs to help bridge that gap.


## Talk to Me Goose

![Agent dale cooper is driving a car while recording a note on a hand-held recorder.](/images/twin-peaks-recorder.webp)
_A still from Twin Peaks where Agent Dale Cooper is recording a note for his secretary, Diane._

Now imagine -- rather than writing up, organizing, and distributing documentation -- you just had to record quick, extemporaneous notes.

They don't have to be recorded, they could also be written, but the key is that they're quick -- and most of the time it's faster to speak than to write. You don't have to break out of the flow of your work.

This tool could have access to some context about you (e.g. your role, your projects, your team, your calendar, etc.), it could also have context about what you're doing (e.g. "you're currently working in an IDE, on repo X, and you have file Y open"), and it could have the context of previous notes you've created. Using that information, it could generate a small update / note / log-entry which could be queried later (e.g. an auto-generated update for your stand-up) or used to update other documentation (I'm picturing something like an event-sourced real-time database -- where these notes are the events and the documentation is generated by replaying these updates).

If you're thinking this sounds like just another note-taking or activity tracking app, I get it, but I think there's a key difference. The goal is not to create a comprehensive record of everything you've done or every site you've visited. The artifact being stored is the note. At most the contextual data would help enrich the model's understanding in the moment. So you could say something like "this email" or "the error on this line" and the model could understand what you mean.


## The Five(-ish) Whys

![A curious taxidermied cat diptych. Aka the "Persian Cat Room Guardian" meme.](/images/why-meme.jpg)

So far we've just discussed a context-aware note-recorder. What about the interview side of things?

This is where I think LLMs have the potential to shine. Up to this point we've been relying on LLMs to answer our questions, but what if instead they asked the questions?

Back to our examples, imagine you just fixed a bug in your codebase and you're logging a note. You start recording. You highlight a line of text and explain that there was an error in this line of code. Your AI-note-taker can *see* the repo you're working on, the file your in, and the line you have highlighted. But there's missing information. What is the error? How did you fix it? What are the implications? Etc.

What if, rather than a static note-taker, it could be an active participant? It could review the information you've given it, look for gaps in its knowledge, and generate a list of questions to fill in those gaps. You answer the questions. It updates its understanding. And the process can continue until it's happy or until you feel like it has enough information. 

You're having a conversation -- like you would with another person -- and the LLM is trying to understand what you're explaining. 

The process doesn't only have to be triggered by your quick status update. You could also go to the bot and let it ask you a bunch of questions. Imagine you want a more in-depth understanding of a process. You could be "interviewed" by a bot. The process might be similar but whereas in the previous example, you don't want to break your flow and the bot just wants to ask a couple of follow up questions for clarity, in this case it's goal is to build out a more comprehensive, structured piece of documentation.


## That's a Dumb Question

![A screen-grab from the Simpsons, with the family in the car and bart asking "are we there yet?".](/images/are-we-there-yet.webp)

> What could go wrong?

At this point it may sound like I'm suggesting we turn loose a pestersome, AI-noodge that never sleeps and has nothing better to do than ask you a million questions. There's a lot about this that could go wrong.

Let's go over just a couple possible issues.


### Q: Am I just going to be subjected to an infinite stream of bad questions?

If you do, then it won't work. Just as important as it's ability to know what questions to ask is it's ability to know when to stop asking questions.

It needs to have some *situational awareness* -- to gague what questions it needs to ask to fill in the critical gaps in it's knowlege, it needs to be able to prioritize those questions, and it needs to know when to stop asking.

That can be a moving target. If you're giving a short update, maybe it only has time for one or two clarifying questions. Or maybe no questions. Maybe it just needs to make an internal note to follow up later, when you have more time.

If you have more time, then it can really start to fill in gaps in it's knowledge.


### Q: Is this just going to be used to automate away my job?

Again (as with all AI tools), maybe. But it has the potential to help facilitate a lot of information sharing.

![Screenshot of a post saying "Write undocumented unmaintainable code and get a lifetime of employment"](/images/no-comments-job-security-meme.webp)

You could always strip out all of the comments and tests from your code as job security. But you probably don't do that, because you don't want to live like that and you're better off collaborating and empowering yourself and the people you work with.


### Q: What happens when the AI misunderstands something crucial?

While the LLM can do a lot of the leg work, it would still require some human intervention to verify, adjust, and correct the information that's being stored.

Ideally the system would be set up to go back and adjust as it gets new information. That way, if you provide a correction or add a note manually, it won't contradict past information but can be incorporated into it's *"mental model"*.


### Q: Won't this create more work? Why not just write the notes myself?

You **can** just write the notes yourself. The goal is for the tool to help you fill in those notes quickly but if it isn't helpful you shouldn't have to use it.


### Q: What about privacy concerns?

Privacy is tough for AI tools with access to your computer. Just ask [Microsoft Recall](https://www.wired.com/story/microsoft-recall-off-default-security-concerns/).

But the tool could have degrees of access. And while providing access to your computer could be helpful, it certainly isn't necessary.

OpenAI, for example, is [experimenting with providing application access to the ChatGPT desktop app](https://help.openai.com/en/articles/10119604-work-with-apps-on-macos).


## Conclusion

TK

Get some time back. Write more notes. Let the AI transcribe it. If it doesn't understand you, it should ask you some clarifying questions.

Does it work? I'm not sure. But it could gracefully degrade to audio recordings.

What's the alternative? Spending lots of time documenting? Or (more likely) not documenting things?

I'm not saying this is a replacement for good manual documentation but I think it could be a good suplement.

