[
  {
    "id": "algorithmic-color-palettes.md",
    "slug": "algorithmic-color-palettes",
    "body": "\nI was recently working on a project in which I wanted to be able to compare the look and feel of images which led me to look for a way to create color palettes using machine learning.\n\nGenerating a color palette can be thought of as a clustering problem in disguise. We want to partition all of an image's pixels into k different groups that best represent the image.\n\n![Image of RGB Color Space (https://en.wikipedia.org/wiki/File:RGB_Cube_Show_lowgamma_cutout_b.png)](/images/rbg-color-space.webp)\n*[Source Wikipedia](https://en.wikipedia.org/wiki/File:RGB_Cube_Show_lowgamma_cutout_b.png)*\n\nInstead of viewing our image as a grid of pixels — each with a red, green, and blue value — we can think of it as an array of points plotted in [3D color-space](https://en.wikipedia.org/wiki/RGB_color_space). There's a dimension for red, a dimension for green, and a dimension for blue and a point can sit anywhere between `0` and `255` in each dimension.\n\nLet's say we're looking to create a palette of 8 colors to represent each image. That means we want to find 8 clusters or partitions that can give us the best possible representation of each image.\n\nThere are many different clustering algorithms to choose from — each with their own pros and cons — but for the purposes of this project I tried both [K-Means clustering](https://en.wikipedia.org/wiki/K-means_clustering) and [Agglomerative clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering#Agglomerative_clustering_example).\n\nI picked 5 sample stills from the film [Only God Forgives (2013)](https://en.wikipedia.org/wiki/Only_God_Forgives) (using the site [FILMGRAB](http://film-grab.com/)) because of its rich palettes.\n\n![Still from Only God Forgives (2013) with k-means generated palette](/images/palette-only-god-forgives-k-means-1.webp)\n*Still from Only God Forgives (2013), courtesy of FILMGRAB, with k-means generated palette.*\n\nAbove is an example film still and the color palette generated using K-Means. As you can see, the algorithm does a fairly good job of making a nice representative palette without much hyper-parameter tuning. Additionally, it allows you to specify a color palette size — which some other clustering algorithms don't (DBSCAN, for example).\n\nYou can see more examples of generated palettes in my [notebook](https://nbviewer.jupyter.org/github/a-poor/color-palettes/blob/main/color-palettes.ipynb#).\n\n## Algorithmic Challenges\n\nAs it turns out, algorithms like K-Means favor high-volume colors over sparse-but-prominent colors. For example, see the below still.\n\n![Still from Only God Forgives (2013) with k-means generated palette](/images/palette-only-god-forgives-k-means-2.webp)\n*Still from Only God Forgives (2013), courtesy of FILMGRAB, with k-means generated palette.*\n\nThe K-Means palette gets a good approximation of a majority of the colors, but looking at the image, you might expect it to include some blue (such as on the mat or the shorts of the downed boxer) or red (such as on the ropes , or the standing boxer's gloves/attire/etc).\n\nThinking back to how K-Means works, the algorithm tries to partition the colors in the image into *k* groups — represented by *k* average points in color-space. In this case, there isn't enough blue or red in the image to be picked up by the algorithm so they're getting washed out by the other similar, more abundant colors.\n\nIn order to work around this, there are a few things we could try.\n\n![Still from Only God Forgives (2013) with k-means and agglomerative generated palettes.](/images/palette-only-god-forgives-k-means-vs-agglomerative.webp)\n*Still from Only God Forgives (2013), courtesy of FILMGRAB, with k-means and agglomerative generated palettes.*\n\nThe above image shows palettes generated using both K-Means and Agglomerative clustering. Agglomerative clustering chose to include a blue in the palette, though it lost the muted yellow and still didn't include any red.\n\n![Still from Only God Forgives (2013) with k-means generated palettes using RGB and HSV colors](/images/palette-only-god-forgives-k-means-rgb-vs-hsv.webp)\n*Still from Only God Forgives (2013), courtesy of FILMGRAB, with k-means generated palettes using RGB and HSV colors.*\n\nAnother approach is to convert the image's colors from RGB to HSV. RGB represents a color as a combination of the intensities of the red, green, and blue channels while HSV represents a color as the hue (the spectrum of base colors), saturation (the intensity of a color), and value (the relative lightness or darkness of a color) — which you can read more about [here](https://en.wikipedia.org/wiki/HSL_and_HSV). The above image shows palettes generated for the same image using K-Means clustering with RGB colors and HSV colors. As you can see, the HSV approach includes both the blue and the yellow (though still no red).\n\nIn order to further improve the results, some options include combining techniques (i.e. Agglomerative clustering + HSV colors), hyper-parameter tuning, using different algorithms (e.g. DBSCAN), and adjusting the color distance metric (If you're interested, you can read more about color differences [here](https://en.wikipedia.org/wiki/Color_difference)).\n\n## Flask App\n\nAs a final bonus, I decided to create a simple proof-of-concept API for generating color palettes from images. The API takes an image URL as a parameter and will use K-Means to generate a palette.\n\nAdditionally, I found that the website [Coolors](https://coolors.co/) makes it easy to create a color palette URL, so the API can return the color palette as a 2D array of colors or as a URL to a Coolors palette.\n\nFor example, using this image…\n\n![Film still from Only God Forgives (2013)](/images/palettes-only-god-forgives-no-palette.webp)\n*Only God Forgives (2013), courtesy of FILMGRAB*\n\n...the API would produce the following link: https://coolors.co/3c030b-050002-3967cd-152f63-760102-7e504c-110c33-b4d1df\n\nViewed together, you would get...\n\n![Still from Only God Forgives (2013) with k-means generated palette courtesy of Coolors](/images/palettes-only-god-forgives-with-generated-palette.webp)\n*Still from Only God Forgives (2013), courtesy of FILMGRAB, with k-means generated palette courtesy of Coolors.*\n\nYou can check out the Flask app in my GitHub repo, [here](https://github.com/a-poor/color-palettes).\n\n## Conclusion\n\nThis was a short but fun side-project and there's definitely a lot more to explore here. Definitely check out [FILMGRAB](https://film-grab.com/) and [Coolors](https://coolors.co/) if you think you might be interested.\n\nYou can find my code on [GitHub](https://github.com/a-poor/color-palettes) or check out the notebook with [nbviewer](https://nbviewer.jupyter.org/github/a-poor/color-palettes/blob/main/color-palettes.ipynb#) or [Binder](https://mybinder.org/v2/gh/a-poor/color-palettes/main?filepath=color-palettes.ipynb).\n\n",
    "collection": "blog",
    "data": {
      "isDraft": false,
      "title": "Algorithmic Color Palettes",
      "subtitle": "Using Machine Learning to Generate Color Palettes from Images",
      "description": "Using Unsupervised Machine Learning algorithms to generate color palettes from film stills.",
      "image": {
        "src": "/images/color-palette-example.webp",
        "alt": "A photo of mountains with a color palette generated from the image.",
        "caption": "Photo by Trey Ratcliff (Source) with my generated color palette.",
        "captionLink": "https://flickr.com/photos/stuckincustoms/8837173497"
      },
      "tags": [
        "data-science",
        "machine-learning",
        "clustering",
        "color-theory",
        "image-processing"
      ],
      "publishDate": "2020-10-13T00:00:00.000Z",
      "recommended": [
        "predict-spotify-skips",
        "big-query-data-augmentation"
      ]
    }
  },
  {
    "id": "apoor-dot-dev.md",
    "slug": "apoor-dot-dev",
    "body": "\nRust is an exciting language with a lot of promise -- fast, safe programs with a small footprint. I've written a few things here and there in Rust but to get more practice, I decided to create a URL shortener using Rust, Tokio, and Axum.\n\nI [recently rewrote my personal site in Astro](/blog/astro-rewrite) and moved my blog from Medium to my personal site, so what better time to add a URL shortener -- like my own personal bitly or tinyurl -- and it just so happens I had an unused domain, `apoor.dev`. With a URL shortener, rather than needing to remember the URLs for my GitHub, my LinkedIn, my Mastodon, specific blog posts and projects, etc., I can instead direct someone to `apoor.dev/mastodon` or `apoor.dev/linkedin`.\n\nIn addition to creating a functional url shortener, my goal was for the application to be fast and lightweight -- two important features for such a service, both of which are selling points of Rust -- and for it to be easy to update and deploy.\n\n\n## Tech Stack\n\nI had already decided to use Rust and Tokio (the most popular async runtime) but there are several options to choose from when it comes to web frameworks: [Actix](https://actix.rs/), [Rocket](https://rocket.rs/), [Gotham](https://gotham.rs/), [Axum](https://github.com/tokio-rs/axum) (just to name a few). They are all solid  options but I ultimately decided to go with Axum -- the newest of the group and part of the Tokio package ecosystem.\n\nIn order to create a fast, easily deployable application, I decided to use Fly.io as a host.\n\nLastly, in an attempt to put some concrete numbers behind the somewhat nebulous performance claims, I decided to perform some load testing on the final application using [k6](https://k6.io), Grafana's open source load testing tool.\n\n\n## Writing a Web Service with Axum\n\nThe web app itself was easy to write. Coming from other language-framework-combos like Node and Express, Go and Echo, or Python and FastAPI, using Axum felt very familiar.\n\nHere's an example handler function to return a simple JSON response:\n\n```rust\nuse axum::Json;\nuse serde_json::{json, Value};\n\nasync fn root() -> Json<Value> {\n    Json(json!({\n        \"id\": 42,\n        \"msg\": \"Hello, World!\"\n    }))\n}\n```\n\nIt's terse and boiler-plate free without being limiting. Handlers get data from the request via [_extractors_](https://docs.rs/axum/latest/axum/#extractors) -- allowing you to pull out headers, request bodies, etc. as function arguments and return data with [_responses_](https://docs.rs/axum/latest/axum/#responses) -- similarly versitile response types allowing you to set the status code, headers, the body, etc. while still allowing you to skip setting values you don't need.\n\nWith the small footprint of this service, though, I wasn't able to get a sense of how well this ease-of-use scales.\n\n\n## Middleware and Logging\n\nThe most-used logging library for Tokio and Axum is [_Tracing_](https://github.com/tokio-rs/tracing), which is also maintained by the Tokio team. Having used it as the logging library in this application as well as a few other toy projects, I would say that I haven't quite gotten the hang of it yet. Since it handles more than just logging, it has a different interface than other logging libraries like [Winston](https://github.com/winstonjs/winston) in NodeJS or [Zap](https://github.com/uber-go/zap) in Go.\n\nLikewise, the Axum middleware package, [Tower](https://github.com/tower-rs/tower) (also under the Tokio umbrella), feels different than other middleware tools. It aims to provide reusable middleware across tools like Axum, Hyper, and Tonic.\n\n\n## Data Storage Decision\n\nMy original intention was to store URL entries -- the mapping from short keys to their respective redirect URLs (eg \"blog\", as in \"apoor.dev/blog\" would link to \"austinpoor.com/blog\") -- in a cache like Redis. Fly.io has an simple [Redis integration via Upstash](https://fly.io/docs/reference/redis/), which would be a perfect option. But as I was building the app, I decided it would be simpler and faster to store the data as a `HashMap` in the application itself.\n\nSince I don't often make changes to the URLs and, when I do, a GitHub action is able to re-deploy the site in less than 5 minutes, there's no need to over-engineer and complicate things by adding an external dependancy.\n\nIn a future iteration, I do plan to add a cache to monitor request volume but in the meantime, a `HashMap` gets the job done quickly and easily.\n\n\n## Building and Deploying to Fly.io\n\nCompared to Go, the rust compilation step takes a little while, even for a small app like this one. The final release build comes in at about `6.5MB`, which feels a bit on the large side. To get a better sense of the file-size breakdown I used the tool [cargo-bloat](https://crates.io/crates/cargo-bloat) and got the following results:\n\n```sh\n$ cargo bloat --release\n    Finished release [optimized] target(s) in 0.05s\n    Analyzing target/release/apoor-dot-dev\n\n File  .text     Size       Crate Name\n 1.3%   6.5%  86.5KiB  tower_http <tower_http::trace::on_response::DefaultOnResponse as tower_http::trace::on_respons...\n 0.5%   2.2%  29.9KiB  tower_http <tower_http::trace::on_failure::DefaultOnFailure as tower_http::trace::on_failure::...\n 0.3%   1.7%  22.3KiB         std addr2line::ResDwarf<R>::parse\n 0.3%   1.4%  19.2KiB         std std::backtrace_rs::symbolize::gimli::resolve::{{closure}}\n 0.3%   1.4%  18.1KiB       hyper <hyper::proto::h1::role::Server as hyper::proto::h1::Http1Transaction>::encode\n 0.2%   1.1%  14.6KiB       hyper hyper::proto::h1::dispatch::Dispatcher<D,Bs,I,T>::poll_loop\n 0.2%   0.9%  12.0KiB       hyper hyper::proto::h1::decode::Decoder::decode\n 0.2%   0.9%  11.7KiB       hyper hyper::proto::h1::role::Server::encode_headers_with_original_case\n 0.2%   0.8%  11.1KiB  tower_http <tower_http::trace::make_span::DefaultMakeSpan as tower_http::trace::make_span::Mak...\n 0.2%   0.8%  11.0KiB       hyper <hyper::proto::h1::role::Server as hyper::proto::h1::Http1Transaction>::parse\n 0.2%   0.8%  10.8KiB         std addr2line::ResUnit<R>::parse_lines\n 0.2%   0.8%  10.5KiB        http http::header::name::StandardHeader::from_bytes\n 0.1%   0.6%   8.2KiB miniz_oxide miniz_oxide::inflate::core::decompress\n 0.1%   0.6%   8.0KiB       hyper hyper::proto::h1::io::Buffered<T,B>::poll_flush\n 0.1%   0.5%   6.7KiB       hyper hyper::proto::h1::conn::Conn<I,B,T>::poll_read_head\n 0.1%   0.5%   6.1KiB       hyper <hyper::server::tcp::AddrIncoming as hyper::server::accept::Accept>::poll_accept\n 0.1%   0.4%   5.9KiB       tokio tokio::runtime::park::CachedParkThread::block_on\n 0.1%   0.4%   5.9KiB        axum <F as axum::handler::Handler<(M,T1),S,B>>::call::{{closure}}\n 0.1%   0.4%   5.8KiB         std gimli::read::abbrev::Abbreviations::insert\n 0.1%   0.4%   5.8KiB         std <core::pin::Pin<P> as core::future::future::Future>::poll\n15.2%  74.3% 995.9KiB             And 4572 smaller methods. Use -n N to show more.\n20.5% 100.0%   1.3MiB             .text section size, the file size is 6.4MiB\n```\n\nI initially would have guessed that the Tokio runtime might be the largest factor responsible for the size of the binary but, according to _cargo bloat_, `tower` and `hyper` took up the most space -- but even still, the largest item only took up `86KB`.\n\nDeploying the app to Fly.io was a breeze. Even though Fly doesn't provide language support for Rust specifically, it's easy enough to deploy an app using a `Dockerfile`. \n\nI used a multi-stage build to keep the final container size small. I originally used the [Debian image](https://hub.docker.com/_/debian), `buster-slim`, which had a final image size of about `75MB` but, after switching to [Alpine](https://hub.docker.com/_/alpine), I was able to get the image size down to about `13MB`.\n\nI _did_ run into one hiccup with the Rust/Alpine image. My build was failing with the following error:\n\n```\n error: linking with `cc` failed: exit status: 1\n ```\n\nAfter doing some digging and with the help of [this](https://github.com/rust-lang/rust/issues/25289) GH issue, I was able to get it running by adding the following line to my `Dockerfile`:\n\n```\nRUN apk add --update alpine-sdk\n```\n\n## Load Testing and Performance\n\nOnce I had the API up and running, my next step was to do some load testing. \n\n[k6](https://k6.io/) is a tool by Grafana Labs, written in Go, for load testing web applications. It allows you to define a set of rules, in JavaScript, for _virtual users_ to access your web service and reports statistics like latency, failure rates, etc.\n\n![A GIF showing the terminal output from running k6](/images/apoor-dot-dev-load-test-demo.gif)\n_A GIF showing the terminal output running the k6 load-test of my URL shortener._\n\nI ran a basic test (see above) against a locally running version of the app, that would randomly request an endpoint from a set of both valid and invalid paths. The result was a median response time of about `25ms` and a P90 response time of about `50ms`. While I was hoping for single-digit response times, that's still plenty fast to get the job done.\n\n\n## Oops! I DDoSed Myself...\n\nWhile I was getting my load test up and running, experimenting with the `k6` configuration, and running it against my local version of the URL shortener, I noticed some unexplained errors being returned. I ran the load test again a few times, tweaking the settings, just to be sure but I was still getting the errors.\n\nI came to realize that the errors were happening because the URL shortener was doing exactly what it was supposed to do...redirecting the user...and k6 was doing exactly what _it_ was supposed to do...follow redirects. So in the process of load testing my local version of the URL shortener, k6 was innocently and inadvertently following those redirects to...my GitHub, my LinkedIn, and...my personal site, which I recently [rewrote in astro](/blog/astro-rewrite) and hosted on Vercel. Vercel noticed the suspicious traffic and blocked the potentially malicious IP address...my IP address...blocking me from accessing my own site. Oops! Fortunately, Vercel support was very understanding and restored my access.\n\nThe moral of the story is...load test with caution and watch out for redirects!\n\nIn case you're curious, you can prevent redirects in k6 with the following: \n\n```\nhttp.get(\"...\", { redirects: 0});\n```\n\n\n## Next Steps\n\nI'm pretty happy with the current version of the app but if I'm looking to do some more tinkering in the future, I'd like to add some metrics to better track usage. I might also like to take a crack at improving the performance or shrinking the binary size.\n\n\n## Conclusion\n\nI had a lot of fun with this experiment. Axum was easy to use and will definitely be my first choice next time I'm creating a web service with Rust. \n\n\n## Reference\n\n- [apoor-dot-dev GitHub Repo](https://github.com/a-poor/apoor-dot-dev/)\n- [axum](https://github.com/tokio-rs/axum)\n- [Tokio](https://tokio.rs/)\n- [k6](https://k6.io/)\n- [cargo-bloat](https://crates.io/crates/cargo-bloat)\n\n",
    "collection": "blog",
    "data": {
      "isDraft": false,
      "title": "Making apoor.dev",
      "subtitle": "Building a URL Shortener with Rust, Tokio, and Axum.",
      "description": "I recently built a URL shortener using Rust, Tokio, and Axum, and in this blog post, I'm sharing my experience. From selecting the right tech stack to load testing the application, I cover everything that went into building this service.",
      "image": {
        "src": "/images/dalle-crab-in-a-hot-baloon.webp",
        "alt": "DALL-E 2's take on Ferris the crab (Rust's mascot) in a hot air baloon (Fly.io's logo)",
        "caption": "DALL-E 2's take on Ferris the crab (Rust's mascot) in a hot air baloon (Fly.io's logo)"
      },
      "tags": [
        "rust",
        "async",
        "url-shortener",
        "axum",
        "tokio",
        "fly-io"
      ],
      "publishDate": "2023-04-18T00:00:00.000Z",
      "recommended": [
        "astro-rewrite",
        "js-in-rs"
      ]
    }
  },
  {
    "id": "astro-rewrite.md",
    "slug": "astro-rewrite",
    "body": "\nAstro has been generating a lot of buzz recently, as a web framework that let's you writing fast websites that ship minimal JavaScript (and uses something called an [Island Architecture](https://docs.astro.build/en/concepts/islands/)) while still allowing you to selectively use familiar UI tools like React, and with first class support for Markdown. \n\nI decided to see what all the fuss was about by re-writing my personal website using Astro. I'll walk you through how it went.\n\n\n## Why Did I Rewrite My Site?\n\nFirst and foremost, I wanted an excuse to try out Astro but I also wasn't particularly happy with my current site. \n\nMy site started out as a single page, written in React — not just an SPA but actually just a single page — basically an HTML resume. And it stayed like that for a while.\n\nThen I rewrote it in SvelteKit. For those unfamiliar, SvelteKit is to Svelte what NextJS is to React (if that helps). I enjoyed using SvelteKit it shares a lot of the same benefits as Astro, compared to frameworks like React — it’s fast and it tries to ship minimal JavaScript.\n\nThe one key downside that I did see with SvelteKit compared to Astro was the markdown support. Astro has great, out-of-the-box support for markdown and SvelteKit doesn’t. I found myself writing custom Svelte components to handle images, code blocks, links, etc. which meant adding an extra step in the blog-writing process. It was nothing complicated but I wanted to clear as many unnecessary mental road-blocks as possible.\n\nAt the end of the day I was happy with SvelteKit, I was just looking for an excuse to try Astro, and a personal site seems like an easy place to try out new tools. The content is consistent and (hopefully) familiar, so it makes it easy to rewrite and re-rewrite* and re-re-re-…-write.\n\n\n## Why Astro?\n\n![A DALL-E generated image of a paper diorama showing a rocketship (the Astro framework logo) flying through space. ](/images/astro-rewrite-dalle.webp)\n_A paper diorama of a rocketship flying through space, generated by DALL-E._\n\nAstro ends up being a pretty natural fit for a personal site like mine. There’s minimal interactivity so Astro allows you to define and re-use components, like you would with React, but then can compile the results to HTML with little-to-no JavaScript. This allows an Astro site to start up quickly, whereas a React-based site needs to load a heavy JavaScript-based runtime when the site first loads, before it can display any content or become reactive. And that issue can multiply if you’re using React on it’s own, as a single page application, where it might need to load the runtime and then all of your content on all of your pages before staring up.\n\nAstro’s added benefit is it’s strong support for rendering markdown and MDX, making it a lot easier to write blog posts, preventing me from using that as an excuse for not creating more content.\n\nAnother fun benefit of Astro is it’s ability to include components from other frameworks like React, Vue, Svelte [and more!](https://docs.astro.build/en/guides/integrations-guide/#official-integrations), and even intermix those framework components. When I started building my Astro site I experimented a bit with adding Svelte and React components but ultimately couldn’t find a good reason to use them over regular Astro components, and need to pay the resulting performance penalty. Though I can see that feature being helpful if I wanted to include some functionality from, say, a React library that I didn’t want to have to re-create — for example, maybe I wanted to add a cool custom visualization using React and AirBnB’s [visx](https://airbnb.io/visx/) library, it would be pretty simple to add to just one or a few pages using Astro.\n\n\n## Working with Astro\n\nAstro was easy to work with. It had a very gentle learning curve and it felt very familiar coming from frameworks like React and Svelte.\n\nIn addition to Astro, I used Tailwind for styling. (I know Tailwind is a divisive CSS framework so I’ll leave it at that.)\n\nMost of my pages were `.astro` files, and then I have two *Content Collections* (a new feature in Astro v2) for my blog posts and my projects. The pages in my content collections were markdown files with custom frontmatter. My blog posts then also had Astro components defining how they should be formatted — wrappers, adding SEO tags, wrapper HTML layouts (pulling data from the frontmatter), and applying CSS styles to the translated markdown.\n\nThere is also a collection of markdown files for my projects page, though I’m currently only using the frontmatter and redirecting to the respective GitHub links for each project. Still, I liked the content collection approach, rather than defining the same data in JSON and it leaves a good opening in case I want to add some markdown describing each project.\n\n\n## Deploying the Site\n\nThe decision of where to deploy the site is made a lot simpler by the fact that it's able to be statically compiled. My site was originally hosted on GitHub Pages, then Netlify, so I decided to change hosting providers, too (why not!), and move to Netlify.\n\nAstro provides a simple Netlify integration which, combined with Netlify’s GitHub integration, makes the deployment quick and painless! The builds are fast and usually take about 10-20s from when I push a change to GitHub to when the code is live on my site.  \n\n\n## Analytics, SEO, and Site Performance\n\n![A screenshot of the Google Page Speed Insights for my site.](/images/apdc-page-speed-screenshot.webp)\n_A screenshot of the [Google Page Speed Insights for my site](https://pagespeed.web.dev/report?url=https%3A%2F%2Faustinpoor.com)._\n\nSEO was something I neglected to add to previous iterations of my site but which was pretty simple using Astro. I created a couple of Astro components with meta tags to place in the `<head>` using data from the page (e.g. frontmatter data on blog post pages). \n\nIn terms of performance, again, Astro got me pretty far right out-of-the-box. There is a 1st party plugin to help with resizing images but I ended up just resizing them myself and converting them to `webp` format.\n\nAfter that, the main ding on my Google Page Speed scores came from the loading of the Google Analytics script.\n\nThere is also an Astro plugin for Partytown — a tool for running resource-intensive scripts in the background using web workers — but the setup was to cumbersome between Partytown, Astro, Google Analytics, and Vercel so instead I opted to move to a smaller (\"< 1 KB\"), more privacy friendly, and open-source (though I opted to use the hosted version) analytics solution, Plausible. I’ve been very happy with Plausible — it loads quickly and has everything I need to keep track of my site’s usage without feeling overloaded.\n\n\n## Future Work\n\nMy main goal will be to write more blog posts, more consistently. Other than that there’s plenty of room to clean things up and smooth out the design. In particular, I’d like to make the “About Me” page less of a wall of text and a bit more fun.\n\nAnd who knows, maybe a new framework will come along and I’ll have to re-re-re-re-re-write the whole site.\n\n\n## Conclusion\n\nAstro is fast and easy to use, and it has great markdown support. It was a lot of fun to work with, and I'm looking forward to using it on future projects.\n\nIt may not be the best fit for sites with a lot of dynamic content (e.g. if you’re writing the next Figma) but for a personal site, a blog, or project documentation, Astro is a great fit.\n\nAnd if you’re tired of deploying the 800-lb gorilla of web-analytics, Google Analytics, when you just want to know how many people read your blog post, consider give Plausible a try!\n\n\n## Bonus: Oops! I just DDoS-ed myself.\n\nI’ll hopefully wrote a separate blog post about this but, just as a quick side-note / bonus / blooper-reel...\n\nSince my Astro re-write, I created a small and simple URL shortener in Rust, for fun and to get more experience with the language. It’ll be a fun project and it’ll make it easier for me to direct people to my GitHub or my LinkedIn or my personal site.\n\nAfter having heard about the great, bear-metal performance of Rust services, I was excited to try it out for myself.\n\nI even thought it would be a great opportunity for me to test that great performance by using the Grafana load-testing tool, k6. I ran my load-test on a local version of the Rust app but couldn’t figure out why I was getting error messages. That is until I realized that I forgot to prevent k6 from following the redirects along to my personal site.\n\nThe result was I ended up getting my IP address blocked by Vercel for my own site and I had to email their support team and explain what happened. Fortunately, they were very understanding and restored my access.\n\nSo the moral of the story here is, double check that you’re 100% sure what you’re load-testing!\n\n\n## References\n\n- [My personal site](https://austinpoor.com)\n- [Personal Site Source Code](https://github.com/a-poor/austinpoor-dot-com)\n- [Source Code for Old React Site](https://github.com/a-poor/austinpoor-dot-com/releases/tag/old%2Freact)\n- [Source Code for Old SvelteKit site](https://github.com/a-poor/austinpoor-dot-com/releases/tag/old%2Fsvelte-kit)\n- [Source Code for Old (unfinished) NextJS Site](https://github.com/a-poor/austinpoor-dot-com/releases/tag/old%2Fnext)\n- [Astro Docs](https://astro.build/)\n- [Plausible Docs](https://plausible.io/)\n- [Tailwind Docs](https://tailwindcss.com/)\n- [Page Speed Insights for My Site](https://pagespeed.web.dev/report?url=https%3A%2F%2Faustinpoor.com)\n- [Astro Content Collections](https://docs.astro.build/en/guides/content-collections)\n- [WebP Images](https://developers.google.com/speed/webp)\n- [Partytown](https://partytown.builder.io/)\n- [k6 Docs](https://k6.io/)\n\n\n",
    "collection": "blog",
    "data": {
      "isDraft": false,
      "title": "Re-Re-Rewriting My Site in Astro",
      "subtitle": "Using the Astro Framework to Make a Simpler, Faster Site",
      "description": "Recapping the process of rewriting my personal site and blog using the new Astro framework.",
      "image": {
        "src": "/images/apdc-site-screenshot.webp",
        "alt": "A screenshot of the home page of my personal site, rewritten in Astro.",
        "caption": "A screenshot of the home page of my personal site, rewritten in Astro."
      },
      "tags": [
        "blog",
        "javscript",
        "astro",
        "framework",
        "frontend"
      ],
      "publishDate": "2023-03-05T00:00:00.000Z",
      "recommended": [
        "apoor-dot-dev",
        "js-in-rs"
      ]
    }
  },
  {
    "id": "big-query-data-augmentation.md",
    "slug": "big-query-data-augmentation",
    "body": "\nGoogle Cloud's BigQuery is a great tool for data scientists to quickly and easily augment their datasets with external data. Specifically, BigQuery has a [listing of public datasets](https://cloud.google.com/bigquery/public-data) from a variety of different sources. All you need is a Google Cloud account and some basic SQL knowledge.\n\nHere are just a few useful public datasets:\n\n- [US Census American Community Survey](https://console.cloud.google.com/marketplace/product/united-states-census-bureau/acs)\n- [COVID-19 Data from Google](https://console.cloud.google.com/marketplace/product/bigquery-public-datasets/covid19-public-data-program)\n- [NOAA Global Weather Data](https://console.cloud.google.com/marketplace/product/noaa-public/gsod)\n- [Public Domain Artworks from The Met](https://console.cloud.google.com/marketplace/product/the-metropolitan-museum-of-art/the-met-public-domain-art-works)\n- [Bitcoin Transaction Data](https://console.cloud.google.com/marketplace/product/cmorqs-public/cmorq-bcd-data)\n\n## A Quick Example\n\nI think one of the most useful among the BigQuery public datasets is the US Census ACS data, which gives multi-year data broken down geographically (by state, zip code, county, etc.).\n\nIt has a lot of great demographic information like population (broken down by age, race, gender, marital status, etc.), education levels, employment, income, and much more.\n\nFor example, say I wanted to query the total population and median household income for three zip codes in the NYC area. There's a table called `zip_codes_2018_5yr` that gives a 5-year estimate of census data for the year 2018, broken down by zip code.\n\nHere's what my query will look like:\n\n```sql\nSELECT \n  geo_id, -- Where geo_id is the zip code\n  total_pop,\n  median_income\nFROM \n  `bigquery-public-data`.census_bureau_acs.zip_codes_2018_5yr\nWHERE \n  geo_id in (\"11377\",\"11101\",\"10708\"); \n```\n\nAnd I can run it in the BigQuery UI...\n\n![Screenshot of the BigQuery UI](/images/bq-screenshot-1.webp)\n\nAnd get the following results...\n\n![Viewing query results in the BigQuery UI](/images/bq-screenshot-2.webp)\n\nGreat! I got my answer in 0.4 seconds and now I can go back and expand my query to get this data for multiple years. Or, I can export the results to a CSV or JSON file to join it up with my data.\n\n![Screenshot showing export options for BigQuery results](/images/bq-screenshot-3.webp)\n\nFinally, as a bonus, you can connect to BigQuery through Python with the package [google-cloud-bigquery](https://googleapis.dev/python/bigquery/latest/index.html).\n\n## Links / Further Reading\n\n- [More About BigQuery](https://googleapis.dev/python/bigquery/latest/index.html)\n- [BigQuery's Public Datasets](https://cloud.google.com/bigquery/public-data)\n- [BigQuery SQL Syntax](https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax)\n- [Querying BigQuery Tables from Python](https://googleapis.dev/python/bigquery/latest/index.html)\n\n",
    "collection": "blog",
    "data": {
      "isDraft": false,
      "title": "Painless Data Augmentation with BigQuery",
      "subtitle": "Quickly Augmenting Your Datasets with BigQuery Public Data",
      "description": "Google Cloud's BigQuery is a great tool for data scientists to easily augment their datasets with external data – using BigQuery's public datasets.",
      "image": {
        "src": "/images/big-query-blocks-dalle.webp",
        "alt": "An image of toy blocks being assembled into a tower by a little crane, generated by DALL-E.",
        "caption": "An image of toy blocks being assembled into a tower by a little crane, generated by DALL-E."
      },
      "tags": [
        "data-science",
        "google-cloud-platform",
        "big-query",
        "sql",
        "data"
      ],
      "publishDate": "2021-01-07T00:00:00.000Z",
      "recommended": [
        "csv-to-postgres-with-pandas",
        "flask-ml-predictions",
        "plots-with-jinja"
      ]
    }
  },
  {
    "id": "command-or-control.md",
    "slug": "command-or-control",
    "body": "\nYou've just built a cool new site and now you want to add search. Maybe GPT-powered vector search, maybe just Algoila. You put in a search button and you want the user to be able to open it with the hotkey `Cmd + K`. But should the hotkey be `Cmd + K` or `Ctrl + K`?\n\nIf your user is on a Mac, it should be `Cmd` (aka \"command\" aka `⌘` ). Otherwise it should be `Ctrl` (aka \"control\" aka `^`). So how can you tell?\n\nOne option is [`navigator.platform`](https://developer.mozilla.org/en-US/docs/Web/API/Navigator/platform), a string identifying the platform on which the browser is running (e.g. `\"MacIntel\"`, `\"Win32\"`, etc.). While it's been supported for quite a while, unfortunately it is now marked as deprecated.\n\n![\"navigator.platform\" MDN support grid](/images/navigator.platform-support-grid.png)\n_\"navigator.platform\" MDN support grid_\n\nPre-deprecation, MDN recommended avoiding using this feature _except_ in this case, when identifying the platform's modifier key.\n\n> `navigator.platform` should almost always be avoided in favor of feature detection. But there is one case where, among the options you could use, `navigator.platform` may be the least-bad option: When you need to show users advice about whether the modifier key for keyboard shortcuts is the `⌘` command key (found on Apple systems) rather than the `⌃` control key (on non-Apple systems)\n\nSee their example reproduced below:\n\n```js\nlet modifierKeyPrefix = \"^\"; // control key\nif (\n  navigator.platform.indexOf(\"Mac\") === 0 ||\n  navigator.platform === \"iPhone\"\n) {\n  modifierKeyPrefix = \"⌘\"; // command key\n}\n```\n\nIf `navigator.platform` is deprecated, what should we use instead?\n\nWell MDN points you to [`navigator.userAgentData.platform`](https://developer.mozilla.org/en-US/docs/Web/API/NavigatorUAData/platform). Here `navigator.userAgentData` is of type [`NavigatorUAData`](https://developer.mozilla.org/en-US/docs/Web/API/NavigatorUAData) which has the properties `platform` (a string identifying the platform running the browser), `mobile` (a boolean set to `true` if running on a mobile device), and `brands` (an array with brand data about the user's browser).\n\nAs a brief aside, MDN notes that those properties are considered \"low entropy\". The `NavigatorUAData` object also contains a method for obtaining \"high entropy\" values from the user-agent – called `getHighEntropyValues` – where low entropy values are unlikely to be able to be used to identify the user and high entropy values are more likely to be usable in identifying the user.\n\n![\"navigator.userAgentData.platform\" MDN support grid](/images/navigator.userAgentData.platform-support.png)\n_\"navigator.userAgentData.platform\" MDN support grid_\n\nUnfortunately, `navigator.userAgentData.platform` is considered experimental and currently is only supported by Chromium-based browsers (Chrome, Edge, Opera).\n\nSo now we have one deprecated API and one experimental API. Are there any other options? Fortunately, yes! The third option is... the [`User-Agent`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent), a string that contains information about how a user is accessing the site (browser type and version, operating system, etc.) and is sent as an HTTP header in requests to the server.\n\nThe good news is that `User-Agent` data is available and well-supported in both the browser (via [`navigator.userAgent`](https://developer.mozilla.org/en-US/docs/Web/API/Navigator/userAgent); see below support table) and on the server side (again, as an HTTP request header). This means, unlike with `navigator.platform` and `navigator.userAgentData.platform`, the `User-Agent` data can be used with a SSR framework like NextJS, rather than needing to wait for the code to run on the client before determining the user's OS.\n\n![\"navigator.userAgent\" MDN support grid](/images/navigator.userAgent-support.png)\n_\"navigator.userAgent\" MDN support grid_\n\nThe bad news is `User-Agent` data can be messy and unreliable. According to MDN:\n\n> Browser identification based on detecting the user agent string is **unreliable** and **is not recommended**, as the user agent string is user configurable.\n\nThat said, while this may not be the most reliable method of _browser_ identification, hopefully the _operating system_ information is a bit more reliable.\n\nMDN has a great article with advice on gleaning information from `User-Agent`, which you can find [here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Browser_detection_using_the_user_agent#os).\n\nCurrently there isn't one consistent way to detect the user's OS. Until `navigator.userAgentData` is fully supported, we may be stuck polyfilling it with the deprecated `navigator.platform` or the unreliable `User-Agent` string. Fortunately, between those three APIs you should be able to get a good enough answer. And worst case, if you guess wrong it isn't the end of the world this is just a progressive enhancement that shouldn't break your site.\n\n![Screenshot from the Astro docs showing the search hotkey is \"/\"](/images/astro-search.png)\n_Screenshot from the [Astro docs](https://docs.astro.build/en/getting-started/)_\n\nIf you _really_ care that much about making sure you don't show your user the wrong modifier key you can always skip `Ctrl+K`/`Cmd+K` and use `/` instead, like Astro does!\n",
    "collection": "blog",
    "data": {
      "isDraft": false,
      "title": "Command or Control",
      "subtitle": "Knowing What Modifier Key to Put in Your Search Bar",
      "description": "Should your hotkeys use \"Cmd\" or \"Ctrl\"? How can you tell if your user is on a Mac or not? This blog post walks through multiple approaches to OS detection in JavaScript.",
      "image": {
        "src": "/images/command-and-control-search-samples.png",
        "alt": "Screenshots from the TailwindCSS and TanStack Query doc sites showing the hotkey \"Cmd + K\" used for opening search.",
        "caption": "Screenshots from the TailwindCSS and TanStack Query doc sites showing the hotkeys used for opening search."
      },
      "tags": [
        "javascript",
        "hotkey",
        "user-agent",
        "javascript-framework",
        "browser-detection"
      ],
      "publishDate": "2023-09-10T00:00:00.000Z",
      "recommended": [
        "js-in-rs",
        "astro-rewrite",
        "apoor-dot-dev"
      ]
    }
  },
  {
    "id": "csv-to-postgres-with-pandas.md",
    "slug": "csv-to-postgres-with-pandas",
    "body": "\nSometimes it can be kind of a pain to deal with copying CSV data into a Postgres database — especially if you don't want to write out a long schema. Why not let Pandas do all that legwork for you? I'll walk you through a quick example using the Iris dataset (here's a [link](https://archive.ics.uci.edu/ml/datasets/Iris) to the data).\n\n## Loading CSV Data Into Postgres with Pandas\n\nAssuming you have a Postgres server up and running, you'll need to either create a database to store your data or use an existing one. Using your preferred Postgres interface (i.e. [psql](https://www.postgresql.org/docs/9.3/app-psql.html) or [pgAdmin](https://www.pgadmin.org/)), you can run the following to create your database…\n\n```sql\nCREATE DATABASE iris;\n```\n\nNow that you have a database to store your table, you can move over to Python to import your data. You'll need the `SQLAlchemy` Python toolkit (if you don't already have it, it can be installed with `pip install SQLAlchemy`).\n\nYou're going to load your CSV data with Pandas and then use SQLAlchemy to pass the data to Postgres. The `SQLAlchemy.create_engine` will need the following 6 pieces of information:\n\n- The **DBMS** — In this case, that will be \"postgres\"\n- A **username** for the database\n- That user's **password**\n- The **hostname** for the Postgres server\n- The **port** for the Postgres server (`5432` by default)\n- The name of the **database** to store the new table\n\nAnd it will be formatted as follows:\n\n```py\ncreate_engine(\"<dbms>://<username>:<password>@<hostname>:<port>/<db_name>\")\n```\n\nHere's how that will look in code...\n\n```py\n# Imports\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\n# This CSV doesn't have a header so pass\n# column names as an argument\ncolumns = [\n    \"sepal_length\",\n    \"sepal_width\",\n    \"petal_length\",\n    \"petal_width\",\n    \"class\"\n]\n\n# Load in the data\ndf = pd.read_csv(\n    \"iris.csv\",\n    names=columns\n)\n\n# Instantiate sqlachemy.create_engine object\nengine = create_engine(\n    'postgresql://postgres:my_password@localhost:5432/iris'\n)\n\n# Save the data from dataframe to\n# postgres table \"iris_dataset\"\ndf.to_sql(\n    'iris_dataset', \n    engine,\n    index=False # Not copying over the index\n)\n```\n\nIf your CSV data is too large to fit into memory, you might be able to use one of these two options…\n\n## Working with Large Datasets: Option 1\n\nOne option would be to use the Pandas `chunksize` argument for `pd.read_csv` which will return a generator that will iterate through rows of the CSV and yield `DataFrames` with the number of rows corresponding to the specified chunksize. Here's what that might look like…\n\n```py\n# Imports\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\n# This CSV doesn't have a header so pass\n# column names as an argument\ncolumns = [\n  \"sepal_length\",\n  \"sepal_width\",\n  \"petal_length\",\n  \"petal_width\",\n  \"class\"\n]\n\n# Instantiate sqlachemy.create_engine object\nengine = create_engine(\n    'postgresql://postgres:my_password@localhost:5432/iris'\n)\n\n# Create an iterable that will read \"chunksize=1000\" rows\n# at a time from the CSV file\nfor df in pd.read_csv(\"iris.csv\",names=columns,chunksize=1000):\n  df.to_sql(\n    'iris_dataset', \n    engine,\n    index=False,\n    # if the table already exists, append this data\n    if_exists='append',\n  )\n```\n\n## Working with Large Datasets: Option 2\n\nAnother option (which is a bit more of a hack) would be to load a few rows of data into pandas — enough for it to infer the column datatypes — and then read in the CSV using psql. Here's what that might look like…\n\n```py\n# Imports\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\n# This CSV doesn't have a header so pass\n# column names as an argument\ncolumns = [\n    \"sepal_length\",\n    \"sepal_width\",\n    \"petal_length\",\n    \"petal_width\",\n    \"class\"\n]\n\n# Load in the data\ndf = pd.read_csv(\n    \"iris.csv\",\n    names=columns,\n    nrows=1000    # load in the first 1000 rows\n)\n\n# Instantiate sqlachemy.create_engine object\nengine = create_engine(\n    'postgresql://postgres:my_password@localhost:5432/iris'\n)\n\n# Save the data from dataframe to\n# postgres table \"iris_dataset\"\ndf.to_sql(\n    'iris_dataset', \n    engine,\n    index=False\n)\n```\n\nThen, once Pandas has created the table you can run the following two lines in `psql` in order to delete those few rows added by Pandas and let Postgres re-import the full data...\n\n```sql\nDELETE FROM iris_dataset;\n\n\\COPY iris_dataset FROM 'downloads/iris.csv' DELIMITER ',' CSV; \n```\n\nHopefully that helps and let me know if you have any other suggested techniques!\n\n",
    "collection": "blog",
    "data": {
      "isDraft": false,
      "title": "Quickly Load CSVs into PostgreSQL Using Python and Pandas",
      "subtitle": "Use Pandas to quickly create and populate a Postgres database",
      "description": "Learn a fast way to use Python and Pandas to import CSV data into a Postgres database. Let Pandas infer data types and create the SQL schema for you.",
      "image": {
        "src": "/images/csv-to-postgres.webp",
        "alt": "A flowchart from a CSV file icon to the Pandas library icon to a Postgres database icon.",
        "caption": "Use Pandas to quickly load your CSV data into a Postgres database"
      },
      "tags": [
        "csv",
        "sql",
        "postgres",
        "pandas",
        "data-science"
      ],
      "publishDate": "2020-02-13T00:00:00.000Z",
      "recommended": [
        "big-query-data-augmentation",
        "predict-spotify-skips"
      ]
    }
  },
  {
    "id": "data-science-profilers.md",
    "slug": "data-science-profilers",
    "body": "\nData scientists often need to write a lot of complex, slow, CPU- and I/O-heavy code — whether you're working with large matrices, millions of rows of data, reading in data files, or web-scraping.\n\nWouldn't you hate to waste your time refactoring one section of your code, trying to wring out every last ounce of performance, when a few simple changes to another section could speed up your code tenfold?\n\nIf you're looking for a way to speed up your code, a profiler can show you exactly which parts are taking the most time, allowing you to see which sections would benefit most from optimization.\n\nA profiler measures the time or space complexity of a program. There's certainly value in theorizing about the big O complexity of an algorithm but it can be equally valuable to examine the real complexity of an algorithm.\n\n*Where is the biggest slowdown in your code? Is your code [I/O bound or CPU bound](https://stackoverflow.com/questions/868568/what-do-the-terms-cpu-bound-and-i-o-bound-mean)? Which specific lines are causing the slowdowns?*\n\nOnce you've answered those questions you'll A) have a better understanding of your code and B) know where to target your optimization efforts in order to get the biggest boon with the least effort.\n\nLet's dive into some quick examples using Python.\n\n## The Basics\n\nYou might already be familiar with a few methods of timing your code. You could check the time before and after a line executes like this:\n\n```\nIn [1]: start_time = time.time()\n   ...: a_function() # Function you want to measure\n   ...: end_time = time.time()\n   ...: time_to_complete = end_time - start_time\n   ...: time_to_complete\nOut[1]: 1.0110783576965332 \n```\n\nOr, if you're in a Jupyter Notebook, you could use the magic `%time` command to time the execution of a statement, like this:\n\n```\nIn [2]: %time a_function()\nCPU times: user 14.2 ms, sys: 41 µs, total: 14.2 ms\nWall time: 1.01 s \n```\n\nOr, you could use the other magic command `%timeit` which gets a more accurate measurement by running the command multiple times, like this:\n\n```\nIn [3]: %timeit a_function()\n1.01 s ± 1.45 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```\n\nAlternatively, if you want to time your whole script, you can use the bash command `time`, like so…\n\n```\n$ time python my_script.py\n\nreal    0m1.041s\nuser    0m0.040s\nsys     0m0.000s \n```\n\nThese techniques are great if you want to get a quick sense of how long a script or a section of code takes to run but they are less useful when you want a more comprehensive picture. It would be a nightmare if you had to wrap each line in `time.time()` checks. In the next section, we'll look at how to use Python's built-in profiler.\n\n## Diving Deeper with cProfile\n\nWhen you're trying to get a better understanding of how your code is running, the first place to start is [cProfile](https://docs.python.org/3/library/profile.html#module-cProfile), Python's built-in profiler. cProfile will keep track of how often and for how long parts of your program were executed.\n\nJust keep in mind that cProfile shouldn't be used to benchmark your code. It's written in C which makes it fast but it still introduces some overhead that could throw off your times.\n\nThere are multiple ways to use cProfile but one simple way is from the command line.\n\nBefore we demo cProfile, let's start by looking at a basic sample program that will download some text files, count the words in each one, and then save the top 10 words from each to a file. Now that being said, it isn't too important what the code does, just that we'll be using it to show how the profiler works.\n\n```py\nimport re\nfrom collections import Counter\nimport urllib.request\n\ndef get_book(url):\n  \"\"\"Load text from a URL\"\"\"\n  with urllib.request.urlopen(url) as response:\n    text = response.read().decode(errors='ignore')\n  return text.lower()\n  \ndef split_words(book):\n  \"\"\"Extract the words with regex\"\"\"\n  return re.split(\"[^A-Za-z]+\",book)\n  \ndef count_words(words):\n  \"\"\"Create a dictionary with word counts\"\"\"\n  return Counter(words)\n  \ndef read_books(urls):\n  \"\"\"For each url in urls, \n  load the book and count the words\"\"\"\n  # Create a place to store word counts\n  word_counts = {}\n  # For each book: load it, count words, store the counts\n  for title, path in urls.items():\n    book = get_book(path)\n    words = split_words(book)\n    counts = count_words(words)\n    word_counts[title] = counts.most_common()[:10]\n  return word_counts\n  \ndef save_results(results,path):\n  \"\"\"Save the results to a text file\"\"\"\n  with open(path,'w') as f:\n    for book, words in results.items():\n      f.write(f\"BOOK: {book}\")\n      for word, count in words:\n        f.write(f\"{' '*8}{word:10s}{count:6d}\")\n    \nurls = {\n  'pride-and-prejudice': \\\n    'https://www.gutenberg.org/files/1342/1342-0.txt',\n\n  'alice-in-wonderland': \\\n    'https://www.gutenberg.org/files/11/11-0.txt',\n\n  'sherlock-holmes': \\\n    'https://www.gutenberg.org/files/1661/1661-0.txt',\n\n  'moby-dick': \\\n    'https://www.gutenberg.org/files/2701/2701-0.txt',\n    \n  'count-of-monte-cristo': \\\n    'https://www.gutenberg.org/files/1184/1184-0.txt'\n}\n\n\noutput_file = \"my-results.txt\"\n\n# Download the books and count words\nresults = read_books(urls)\n\n# Save the results to a text file\nsave_results(results,output_file)\n```\n\nNow, with the following command, we'll profile our script.\n\n```bash\n$ python -m cProfile -o profile.stat script.py\n```\n\nThe `-o` flag specifies an output file for cProfile to save the profiling statistics.\n\nNext, we can fire up python to examine the results using the [pstats](https://docs.python.org/3/library/profile.html#module-pstats) module (also part of the standard library).\n\n```\nIn [1]: import pstats\n   ...: p = pstats.Stats(\"profile.stat\")\n   ...: p.sort_stats(\n   ...:   \"cumulative\"   # sort by cumulative time spent\n   ...: ).print_stats(\n   ...:   \"script.py\"    # only show fn calls in script.py\n   ...: )\n   \nFri Aug 07 08:12:06 2020    profile.stat46338 function calls (45576 primitive calls) in 6.548 secondsOrdered by: cumulative time\nList reduced from 793 to 6 due to restriction <'script.py'>ncalls tottime percall cumtime percall filename:lineno(function)\n     1   0.008   0.008   5.521   5.521 script.py:1(<module>)\n     1   0.012   0.012   5.468   5.468 script.py:19(read_books)\n     5   0.000   0.000   4.848   0.970 script.py:5(get_book)\n     5   0.000   0.000   0.460   0.092 script.py:11(split_words)\n     5   0.000   0.000   0.112   0.022 script.py:15(count_words)\n     1   0.000   0.000   0.000   0.000 script.py:32(save_results)\n```\n\nWow! Look at all that useful info!\n\nFor each function called, we're seeing the following information:\n\n- `ncalls`: number of times the function was called\n- `tottime`: total time spent in the given function (excluding calls to sub-functions)\n- `percall`: `tottime` divided by `ncalls`\n- `cumtime`: total time spent in this function and all sub-functions\n- `percall`: (again) `cumtime` divided by `ncalls`\n- `filename:lineo(function)`: the file name, line number, and function name\n\nWhen reading this output, note the fact that we're hiding a lot of data – in fact, we're only seeing 6 out of 793 rows. Those hidden rows are all the sub-functions being called from within functions like `urllib.request.urlopen` or `re.split`. Also, note that the `<module>` row corresponds to the code in `script.py` that isn't inside a function.\n\nNow let's look back at the results, sorted by cumulative duration.\n\n```\nncalls tottime percall cumtime percall filename:lineno(function)\n     1   0.008   0.008   5.521   5.521 script.py:1(<module>)\n     1   0.012   0.012   5.468   5.468 script.py:19(read_books)\n     5   0.000   0.000   4.848   0.970 script.py:5(get_book)\n     5   0.000   0.000   0.460   0.092 script.py:11(split_words)\n     5   0.000   0.000   0.112   0.022 script.py:15(count_words)\n     1   0.000   0.000   0.000   0.000 script.py:32(save_results)\n```\n\nKeep in mind the hierarchy of function calls. The top-level, `<module>`, calls `read_books` and `save_results`. `read_books` calls `get_book`, `split_words`, and `count_words`. By comparing cumulative times, we see that most of `<module>`'s time is spent in `read_books` and most of `read_books`'s time is spent in `get_book`, where we make our HTTP request, making this script (unsurprisingly) I/O bound.\n\nNext, let's take a look at how we can be even more granular by profiling our code line-by-line.\n\n## Profiling Line-by-Line\n\nOnce we've used cProfile to get a sense of what function calls are taking the most time, we can examine those functions line-by-line to get an even clearer picture of where our time is being spent.\n\nFor this, we'll need to install the `line-profiler` library with the following command:\n\n```bash\n$ pip install line-profiler\n```\n\nOnce installed, we just need to add the `@profile` decorator to the function we want to profile. Here's the updated snippet from our script:\n\n```py\n@profile\ndef read_books(urls):\n  \"\"\"For each url in urls, \n  load the book and count the words\"\"\"\n  # Create a place to store word counts\n  word_counts = {}\n  # For each book: load it, count words, store the counts\n  for title, path in urls.items():\n    book = get_book(path)\n    words = split_words(book)\n    counts = count_words(words)\n    word_counts[title] = counts.most_common()[:10]\n  return word_counts\n```\n\nNote the fact that we don't need to import the `profile` decorator function — it will be injected by `line-profiler`.\n\nNow, to profile our function, we can run the following:\n\n```bash\n$ kernprof -l -v script-prof.py\n```\n\n`kernprof` is installed along with `line-profiler`. The `-l` flag tells `line-profiler` to go line-by-line and the `-v` flag tells it to print the result to the terminal rather than save it to a file.\n\nThe result for our script would look something like this:\n\n```\nWrote profile results to script-profile.py.lprof\nTimer unit: 1e-06 s\n\nTotal time: 5.62982 s\nFile: script-profile.py\nFunction: read_books at line 19\n\nLine #  Hits       Time  Per Hit  % Time  Line Contents\n========================================================\n    19                                    @profile\n    20                                    def read_books(urls):\n    21                                      \"\"\"For each url in urls, \n    22                                      load the book and count the words\"\"\"\n    23                                      # Create a place to store word counts\n    24     1        1.0       1.0    0.0    word_counts = {}\n    25                                      # Per book: load, count words, store counts\n    26     6       15.0       2.5    0.0    for title, path in urls.items():\n    27     5  5038674.0 1007734.8   89.5      book = get_book(path)\n    28     5   431378.0   86275.6    7.7      words = split_words(book)\n    29     5   121321.0   24264.2    2.2      counts = count_words(words)\n    30     5    38429.0    7685.8    0.7      word_counts[title] = counts.most_common()[:10]\n    31     1        0.0       0.0    0.0    return word_counts\n```\n\nThe key column to focus on here is `% Time`. As you can see, 89.5% of our time parsing each book is spent in the `get_book` function — making the HTTP request — further validation that our program is I/O bound rather than CPU bound.\n\nNow, with this *new* info in mind, if we wanted to speed up our code we wouldn't want to waste our time trying to make our word counter more efficient. It only takes a fraction of the time compared to the HTTP request. Instead, we'd focus on speeding up our requests — possibly by making them asynchronously.\n\nHere, the results are hardly surprising, but on a larger and more complicated program, `line-profiler` is an invaluable tool in our programming tool belt, allowing us to peer under the hood of our program and find the computational bottlenecks.\n\n## Profiling Memory\n\nIn addition to profiling the time-complexity of our program, we can also profile its memory-complexity.\n\nIn order to do line-by-line memory profiling, we'll need to install the memory-profiler library which also uses the same @profile decorator to determine which function to profile.\n\n```\n$ pip install memory-profiler\n$ python -m memory_profiler script.py\n```\n\nThe result of running `memory-profiler` on our same script should look something like the following:\n\n```\nFilename: script.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n    19   38.426 MiB   38.426 MiB   @profile\n    20                             def read_books(urls):\n    21                               \"\"\"For each url in urls, \n    22                               load the book and count the words\"\"\"\n    23                               # Create a place to store word counts\n    24   38.426 MiB    0.000 MiB     word_counts = {}\n    25                               # For each book: load it, count words, store the counts\n    26   85.438 MiB    0.000 MiB     for title, path in urls.items():\n    27   72.137 MiB    9.281 MiB       book = get_book(path)\n    28   95.852 MiB   23.715 MiB       words = split_words(book)\n    29   85.438 MiB    0.496 MiB       counts = count_words(words)\n    30   85.438 MiB    0.254 MiB       word_counts[title] = counts.most_common()[:10]\n    31   85.438 MiB    0.000 MiB     return word_counts\n```\n\nThere are currently some [issues](https://github.com/pythonprofilers/memory_profiler/issues/236) with the accuracy of the “Increment” so just focus on the “Mem usage” column for now.\n\nOur script had peak memory usage on line 28 when we split the books up into words.\n\n## Conclusion\n\nHopefully, now you'll have a few new tools in your programming tool belt to help you write more efficient code and quickly determine how to best spend your optimization-time.\n\nYou can read more about [cProfile here](https://docs.python.org/3/library/profile.html), [line-profiler here](https://github.com/pyutils/line_profiler), and [memory-profiler here](https://github.com/pythonprofilers/memory_profiler). I also highly recommend the book High Performance Python, by Micha Gorelick and Ian Ozsvald [1].\n\n## References\n\n[1] [M. Gorelick and I. Ozsvald, High Performance Python 2nd Edition (2020), O'Reilly Media, Inc.](https://www.oreilly.com/library/view/high-performance-python/9781492055013/)\n\n",
    "collection": "blog",
    "data": {
      "isDraft": false,
      "title": "Data Scientists, Start Using Profilers",
      "subtitle": "Find the parts of your algorithm that are ACTUALLY slowing you down",
      "description": "A profiler can show you exactly which parts are taking the most time, allowing you to see which sections to spend time optimizing to speed up your code.",
      "image": {
        "src": "/images/data-science-profile-dalle.webp",
        "alt": "An image of a robot holding a stop-watch, generated by DALL-E.",
        "caption": "An image of a robot holding a stop-watch, generated by DALL-E."
      },
      "tags": [
        "data-science",
        "python",
        "profiling",
        "algorithms",
        "optimization"
      ],
      "publishDate": "2020-08-07T00:00:00.000Z",
      "recommended": [
        "plots-with-jinja",
        "flask-ml-predictions",
        "serve-ml-with-grpc"
      ]
    }
  },
  {
    "id": "flask-ml-predictions.md",
    "slug": "flask-ml-predictions",
    "body": "\nOne of the projects I worked on for the Metis data science bootcamp involved creating an MVP of a Flask app to display movie recommendations to a user (think Netflix home screen).\n\nMy recommendations involved a model prediction combined with a SQL query — and all of this was being done when a request came in, before the response was sent. Come presentation day, loading the main page of the site took about 30 seconds.\n\n```py\n@app.route(\"/\")\ndef main_page():\n  predictions = get_predictions()\n  return render_template(\"index.html\", predictions=predictions) \n```\n\nTo be fair, this was an MVP I created on a short deadline in a _data science_ bootcamp; not a _web-dev_ bootcamp. Still, 30 seconds of wait time is not so good.\n\nAfter graduating, once I had more time, I took a second look at my project to see what I could have improved.\n\nHere are two options I could have explored that would have drastically sped up my page loading time without having to change my prediction algorithm.\n\n## 1) Load the Page, Then Make Predictions\n\nInstead of making predictions before returning to the main page (like in the code above), separate prediction code from the page response code in the Flask app. Return the page without predictions. Then, once the page is loaded, call the API using JavaScript. Here's what the updated Flask app code would look like:\n\n```py\n@app.route(\"/\")\ndef main_page():\n  return render_template(\"index.html\")\n\n@app.route(\"/api/predict\")\ndef api_predict():\n  predictions = get_predictions()\n  return jsonify(predictions)\n```\n\nAnd here's what the JavaScript code would look like:\n\n```js\nfetch(\"/api/predict\")\n  .then(r => r.json())\n  .then(predictions => {\n    // Update the page with predicted\n    // movie recommendations...\n});\n```\n\nThis is a small change to the initial code that can make a big difference to the user. The page can initially load with placeholder images or a loading bar so your user can still interact with your site while they wait for the predictions to load.\n\n## 2) Pass off the Work to Celery\n\nBy running slow processes like ML predictions or complex SQL queries in a Flask response function, you're bogging down your Flask server. This might not be a concern for you, depending on how much traffic you're expecting to get. Maybe this is just a POC or only a few people will be using your service at a time. In that case, just using our API method should be fine. Otherwise, you might want to think about a solution that can scale horizontally.\n\nEnter [Celery](https://docs.celeryproject.org/en/stable) — a python library for creating a “distributed task queue.” With Celery, you can create a pool of workers to handle requests as they come in and it's as easy as adding a decorator to a python function.\n\nWith our new Celery workflow, we'll split the API route into two: one route for scheduling a prediction and another for getting the prediction results.\n\nLet's take a look at the updated Flask snippet:\n\n```py\nfrom celery_predict import celery_app\nfrom celery_predict import celery_predict\n\n@app.route(\"/\")\ndef main_page():\n  return render_template(\"index.html\")\n\n@app.route(\"/api/predict\",methods=[\"POST\"])\ndef api_predict():\n    # Get the request data from the post request\n    print(request.json)\n    data = request.json.get(\"input-data\")\n    # Run the task with Celery\n    task = celery_predict.delay(data)\n    # Return the task's id\n    return {\"status\":\"working\",\"result-id\":task.id}\n\n@app.route(\"/api/get-result/<taskid>\")\ndef get_result(taskid: str):\n    # Get the celery task\n    task = AsyncResult(taskid,app=celery_app)\n    # If the task is done, return the result\n    # otherwise return the status\n    if task.state != \"SUCCESS\":\n        return {\"status\":task.state}\n    return {\"status\":task.state,\"result\":task.get()} \n```\n\nAnd here's the new Celery snippet:\n\n```py\nfrom celery import Celery\n\ncelery_app = Celery(\n    \"predict\",\n    broker=\"redis://redis:6379/0\",\n    backend=\"redis://redis:6379/0\"\n)\n\n@celery_app.task\ndef celery_predict(input_data: dict):\n    # ML prediction code...\n    preds = ...\n    return preds\n```\n\nAnd the updated JavaScript:\n\n```js\nfunction checkForResults(rid = \"\") {\n  // Check if result is ready\n  fetch(`/api/get-result/${rid}`)\n    .then(r => r.json())\n    .then(r => {\n      const stat = r.status;\n      // Check if result is success, failure,\n      // or not ready yet\n      if (stat == \"SUCCESS\") {\n        handleSuccess(r);\n      } else if (stat == \"FAILURE\") {\n        handleError(r);\n      } else {\n        // Wait and try again\n        setTimeout(() => checkForResults(rid), 100);\n      }\n  });\n}\n\nfunction makePrediction() {\n  // Prepare data to make a prediction\n  const data = getInputData();\n  // Schedule prediction & wait for result to be ready\n  postData(\"/api/predict\",{'input-data':data})\n    .then(data => data['result-id'])\n    .then(rid => checkForResults(rid));\t\n}\n```\n\nSo now we start by loading the page then the JavaScript will schedule the model prediction, and continue to check for results until they're ready.\n\nAdmittedly, this has increased the complexity of our solution (you'll need to add a broker like Redis and start up a separate Celery worker process) but it allows us to horizontally scale our app by adding as many Celery workers to our pool as we need.\n\nFor a complete example, check out this GitHub repo: [a-poor/flask-celery-ml](https://github.com/a-poor/flask-celery-ml).\n\n",
    "collection": "blog",
    "data": {
      "isDraft": false,
      "title": "Handling ML Predictions in a Flask App",
      "subtitle": "Don't let long-running code slow down your Flask app",
      "description": "Two suggested design patterns for making machine learning predictions (or handling other long-running tasks) in Flask apps by adding API routes and Celery.",
      "image": {
        "src": "/images/ml-predictions-flask-dalle.webp",
        "alt": "An image of toy robots on an assembly line, generated by DALL-E.",
        "caption": "An image of toy robots on an assembly line, generated by DALL-E."
      },
      "tags": [
        "data-science",
        "python",
        "flask",
        "machine-learning",
        "celery"
      ],
      "publishDate": "2021-02-18T00:00:00.000Z",
      "recommended": [
        "serve-ml-with-grpc",
        "data-science-profilers",
        "js-in-rs"
      ]
    }
  },
  {
    "id": "js-in-rs.md",
    "slug": "js-in-rs",
    "body": "\nWhen I saw that Deno has open source Rust crates for running JavaScript code in a Rust application I wanted to give it a try for myself, so I created a basic, proof-of-concept project called `js-in-rs` to get a feel for using Deno's crates in a Rust program -- specifically the `deno_core` crate.\n\n## What Does it Do?\n\nThe goal of `js-in-rs` is to be a CLI, written in Rust, for filtering files using JavaScript expressions. It's like the tool `grep` except where grep uses regular expressions, `js-in-rs` uses JavaScript.\n\nThe `js-in-rs` CLI takes two arguments -- a path to an input file that will be filtered and a JavaScript expression where the the value `line` will be set to the value of each line in the input file and the expression's truthiness will determine if the line should be printed.\n\nHere's an example of what it might look like in practice:\n\n```sh\njs-in-rs example.txt 'line.length > 5'\n```\n\nIf the contents of the file `example.txt` looked like this:\n\n```\nA\nBBBBBB\nCCC\nDDDDD\nEEEEEEE\n```\n\nThe output of our command would be:\n\n```\nBBBBBB\nEEEEEEE\n```\n\nOnly the lines of the file with more than 5 characters would be printed.\n\nThis is a pretty simple example of a JavaScript filter but due to the versitility of JavaScript, this tool is able to represent much more complex filtering logic. In many cases filters would be very hard if not impossible to express using regular expressions. Not only is JavaScript more readable but it can also be more expressive.\n\nSay you had the following filter conditions:\n- If the line is `foo`, print it\n- If the line has 10-20 characters, excluding leading or trailing spaces, print it\n- Otherwise, don't print it\n\nNow say we have the following input file, `sample.txt`:\n\n```\n foo\nfoo\n  short\n    this is a long line\n this line is too long, though\n```\n\nWe could use the following `grep` command (I'm using the `-P` flag for Perl-compatable regex expressions):\n\n```sh\ngrep -P '^(foo|\\s*.{10,20}?\\s*)$' sample.txt\n```\n\nAnd we would correctly get the following output:\n\n```\nfoo\n    this is a long line\n```\n\nEven in this example, we're running into issues of readability for the regular expression and, as the discerning reader may have noticed, this regex pattern doesn't account for all edge cases (eg trailing spaces).\n\nBy contrast, using `js-in-rs` we could write the command as follows:\n\n```sh\njs-in-rs sample.txt \\\n  'line == \"foo\" || (line.trim().length >= 10 && line.trim().length <= 20)'\n```\n\nI would argue that this is much easier to read than the revious regex pattern but even still, we could take it a step further and split it out into multiple lines -- including variable assignments and comments:\n\n```sh\njs-in-rs sample.txt \"$(cat <<EOF\n{\n  // Is the line \"foo\"?\n  if (line === \"foo\") return true;\n\n  // Is the line's length (excluding lead-/trail-ing ws) in [10,20]\n  const trimLine = line.trim();\n  if (trimLine.length >= 10 && trimLine.length <= 20) {\n    return true;\n  }\n\n  // Otherwise, don't print...\n  return false;\n}\nEOF\n)\"\n```\n\nSure enough this gives us the same result:\n\n```\nfoo\n    this is a long line\n```\n\nNow if you're looking back through your bash history after a month or two, you'll have a much easier time remembering what that command does.\n\nWhat if you wanted to only show lines where at least 50% of the characters in the line are uppercase? I have no idea how to do that using regular expressions. Here's what the JavaScript filter might look like using `js-in-rs`:\n\n```js\nline.length > 0 && (\n  Array.from(line)\n    .map(c => c === c.toUpperCase())\n    .reduce((a, b) => a + b) \n  / line.length) > 0.5\n```\n\nThanks to JavaScript and the Deno runtime, `js-in-rs` is able to be more expressive and more versitile than grep and yet simpler than writing out a full script in JavaScript and running it yourself with Node or Deno.\n\n## Show Me the Rust!\n\nUsing the Deno repository's examples as reference I was able to get a simple example up and running without much of an issue.\n\nAfter parsing the command line arguments and reading in the source file to be filtered, the program creates a single instance of the `deno_core::JsRuntime` that will then be reused throughout the application.\n\n```rust\nlet mut runtime = JsRuntime::new(\n  RuntimeOptions::default(),\n);\n```\n\nIt then iterates through the source file, line-by-line, formatting the filter into a JavaScript expression that defines an anonymous function and calls it using the source file's line as the argument (see the above section's explanation).\n\nThat expression is then evaluated using the JS runtime and the result is captured.\n\n```rust\nlet result = runtime.execute_script(\n  \"matcher.js\",\n  js_matcher.into(),\n);\n```\n\nThe result returned can then be deserialized as a `serde_json::Value` enum which is _expected_ to be a boolean value.\n\n```rust\nlet scope = &mut runtime.handle_scope();\nlet local = v8::Local::new(scope, global);\nlet deserialized_value = serde_v8::from_v8::<serde_json::Value>(scope, local);\n```\n\nAssuming a boolean type _is_ returned, its value will determine if the line should be printed.\n\n```rust\nmatch value {\n  serde_json::Value::Bool(b) => {\n    if b {\n      println!(\"{}\", line);\n    }\n  },\n  _ => return Err(Error::msg(format!(\n    \"JS matcher must return a boolean value!\",\n  ))),\n}\n```\n\nThe `deno_core` crate does a good job of passing along error messages which can come in quite handy. For example, say you accidentially add a semicolon in the middle of your filter expression:\n\n```bash\njs-in-rs src/main.rs \\\n  'line.trim().length > 20 &;& line.trim().length < 50'\n```\n\nYou would get a handy error message like the following:\n\n```\nError: Eval error: Uncaught SyntaxError: Unexpected token ';'\n    at matcher.js:1:39\n```\n\nAdmitidly the line numbers may not match up completely, given the fact that the Rust application inserts the filter into a larger expression and error references the file `matcher.js` which isn't real, but it is a good enough starting point to debug the issue.\n\n## So...can I `\"rm /usr/bin/grep\"`?\n\nBy now you're probably all-in on `js-in-rs` and ready to delete `grep` entirely but before you do, remember that this is just a proof of concept. It's very light on features, light on testing, and there's room for improvement on the performance. \n\nEven if this project were to continue on to add features, add tests, and boost the performance, you may be better off using a tool written entirely in Rust or entirely in JavaScript. \n\nRathar than embedding a JavaScript runtime in Rust, the whole tool could be written in JavaScript and [compiled into a self-contained executable using Deno](https://deno.com/manual@v1.32.5/tools/compiler). Though, with that said, the compiled version of `js-in-rs` ends up being about half the size of the pure-JS Deno-compiled version -- the release build of `js-in-rs` is `54 MB`, `grep` is `179 KB`, and a basic _hello world_ JS application compiled using Deno is `103 MB`.\n\n## Takeaways, Recommendations & Conclusions\n\nWorking with Deno in rust was a lot of fun. The documentation was a bit sparce but what they do have, combined with the examples they provide, was enough to get me up and running to create this small PoC that I'm calling `js-in-rs`.\n\nWhile I don't expect `js-in-rs` to unseat `grep` as the go-to command line tool for filtering text -- and don't even really plan to continue developing it -- the experience was more than enough to pique my interest and get me thinking about all kinds of other possible applications for running JavaScript in Rust using Dino. \n\nRust is fast and safe but a bit slower to write and with a bit steaper of a learning curve while JavaScript is well-known language that's fast to write but that generally runs more slowly -- the two languages can fit together synergistically. \n\nHere are a few possible applications that could benefit from running JavaScript in Rust:\n\n- Web-Servers/APIs with the power and performance of Rust that can be customized using JavaScript\n- Data pipelines (eg Apache Airflow) where the orchestration is handled by Rust but the high-level business logic is defined using JavaScript\n- User Defined Functions (UDFs) for databases where Rust can run JS functions in a safe sandbox\n\nThe Deno runtime also has an interesting approach to permissions where it allows users to turn on/off runtime features like network access, file system access, FFI access, environment variable access, etc. to help prevent nefarious code from accessing resources it shouldn't. While I didn't get a chance to explore it in this application, it's on my list to try in the future.\n\nIf you found this interesting I highly recommend trying it out yourself. Play around with Deno's Rust crates, create your own applications that integrate JavaScript, and if you can, document it to help build the collective knowlege base!\n\n## References\n\n- [`js-in-rs` GitHub Repo](https://github.com/a-poor/js-in-rs)\n- [Deno Runtime](https://deno.com/runtime)\n- [Deno GitHub Repo](https://deno.com/runtime)\n- [`deno_core` Docs](https://docs.rs/deno_core/latest/deno_core/)\n- [`deno_core` Examples](https://github.com/denoland/deno_core/tree/main/core/examples)\n- [Deno Compile](https://deno.com/manual@v1.32.5/tools/compiler)\n\n",
    "collection": "blog",
    "data": {
      "isDraft": false,
      "title": "Running JavaScript in Rust with Deno",
      "subtitle": "Exploring Deno's JavaScript Runtime in a Proof-of-Concept Rust Application for Filtering Text with JS Expressions",
      "description": "Explore how the Deno runtime can enable JavaScript code execution in a Rust application via a proof-of-concept project called \"js-in-rs\". Discover the flexibility and versatility of using JavaScript expressions as filters in a CLI, written in Rust, as a powerful alternative to regular expressions. Watch out grep!",
      "image": {
        "src": "/images/dalle-deno-dino-with-crab.webp",
        "alt": "The Deno dinosaur sitting in a field in Pangea -- with a small Rust colored crab in the foreground (Generated by DALL-E)",
        "caption": "The Deno dinosaur sitting in a field in Pangea -- with a small Rust colored crab in the foreground (Generated by DALL-E)"
      },
      "tags": [
        "rust",
        "javascript",
        "deno",
        "cli",
        "grep"
      ],
      "publishDate": "2023-05-03T00:00:00.000Z",
      "updateDate": "2023-08-05T00:00:00.000Z",
      "recommended": [
        "zero-trust-programming",
        "apoor-dot-dev",
        "serve-ml-with-grpc"
      ]
    }
  },
  {
    "id": "plots-with-jinja.md",
    "slug": "plots-with-jinja",
    "body": "\nHave you ever spent way too long digging through documentation, trying to get your plots to look the way you want? Next time it happens, consider ditching matplotlib or seaborn and using [Jinja](https://jinja.palletsprojects.com) to create a custom plot.\n\nJinja is a Python templating language, based on Django templates, which you may be familiar with from other packages like Flask or Airflow. It has a ton of useful features but you only need some basics to get started.\n\nYou can use Jinja to make your visual as an SVG (with a workflow that's pretty similar to D3's) and display them, all while staying in Jupyter.\n\n## Some Basic SVG\n\nBefore we get into the big stuff, let me cover some basics. SVG stands for *Scalable Vector Graphics*, meaning all the image data is stored as points in vector space and therefore can be resized without getting pixelated (unlike *raster* images).\n\nSVGs are XML-based and have many different tags, representing different paths and shapes. Some key tags are:\n\n- `circle`: Draws a circle using *xy* coordinates and a radius\n- `rect`: Draws a rectangle using *xy* coordinates for upper-left corner plus *width* and *height*\n- `polyline`: Draws a path of connected *xy* points\n- `text`: Draws text\n\nEach SVG element has its own attributes that can be changed but most elements share common attributes to control the *fill* and *stroke*.\n\nOne other key thing to keep in mind when creating SVGs is that the *xy* coordinates start at *(0,0)* in the *upper-left corner* and increase as they move towards the *bottom-right corner*.\n\nFor more information on working with SVGs check out Mozilla's reference material [here](https://developer.mozilla.org/en-US/docs/Web/SVG/Tutorial/Introduction).\n\nNow on to Jinja…\n\n## A (Mini) Intro to Jinja Templates\n\nJinja's templates have great features like loops, conditionals, and template inheritance, and that's just scratching the surface; there is a ton of functionality.\n\nHere's a basic example of a Jinja template:\n\n```jinja\n{% for name in names %}\n  Hello, {{ name }}, welcome to {{ place }}!\n{% endfor %} \n```\n\nAnd if you were to render the template with the following...\n\n```py\nnames = [\"tim\", \"sue\", \"tom\", \"sally\"]\nplace = \"my party\"\n```\n\n...you would get the following output...\n\n```\nHello, tim, welcome to my party!\nHello, sue, welcome to my party!\nHello, tom, welcome to my party!\nHello, sally, welcome to my party!\n```\n\nFor more information on Jinja templates, check out the [docs](https://jinja.palletsprojects.com/en/2.11.x/templates/).\n\nNow, let's move on to building a plot with Jinja.\n\n## Plotting with Jinja\n\nLet's make a simple demo scatter plot with some example data, using Jinja.\n\nFor starters, here's the data and some basic layout info I'll use in my template:\n\n```py\ndata = {\n    \"title\":\"Time-Price Comparison\",\n    \"subtitle\":\"Scatter plot of time vs price.\",\n    \"data\":[\n        {\"time\":2,\"price\":1,\"callout\":False},\n        {\"time\":3,\"price\":2,\"callout\":False},\n        {\"time\":4,\"price\":3,\"callout\":True},\n        {\"time\":5,\"price\":4,\"callout\":True},\n        {\"time\":6,\"price\":5,\"callout\":False}\n    ],\n    \"xlabel\":\"Time (PM)\",\n    \"ylabel\":\"Price\",\n    \"caption\":\"Note: Data made up from my imagination\"\\\n        \" and therefore not real. [2020]\"\n}\nlayout = {\n    \"data\": {\n        \"time_min\":1,\n        \"time_max\":7,\n        \"price_min\":0,\n        \"price_max\":6\n    },\n    \"plot\": {\n        \"xmin\":  80,\n        \"ymin\": 110,\n        \"xmax\": 565,\n        \"ymax\": 300,\n        \"pad\":  10,\n        \"point_radius\": 5\n    },\n    \"color\": {\n        \"color_on\": \"hsl(230,70%,60%)\",\n        \"color_off\": \"hsl(0,0%,50%)\",\n        \"axis\": \"hsl(0,0%,30%)\",\n        \"title\": \"hsl(0,0%,0%)\",\n        \"subtitle\": \"hsl(0,0%,50%)\",\n        \"caption\": \"hsl(0,0%,50%)\",\n        \"background\": \"hsl(0,0%,95%)\"\n    }\n}\n```\n\nNow let's make the template, which will be a mix of SVG's XML-style tags and special Jinja commands:\n\n```xml\n<svg\n    xmlns=\"http://www.w3.org/2000/svg\" \n    xml:lang=\"en\" \n    xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n    viewBox=\"0 0 600 400\" \n    width=\"600\">\n    <!-- Set the background color -->\n    <rect \n        id=\"background\" \n        x=\"0\" \n        y=\"0\" \n        width=\"600\" \n        height=\"400\" \n        fill=\"{{ layout.color.background }}\" \n        />\n\n    <!-- Create the Scatterplot -->\n    <g id=\"plot-layer\">\n    {%- for p in data.data %}\n      <circle \n          cx=\"{{ xscale(p.time) }}\" \n          cy=\"{{ yscale(p.price) }}\" \n          r=\"{{ layout.plot.point_radius }}\"\n          {% if p.callout -%}\n              fill=\"{{ layout.color.color_on }}\"\n          {% else -%}\n              fill=\"{{ layout.color.color_off }}\"\n          {% endif -%}\n          />     \n    {%- endfor %}\n    </g>\n    \n    <!-- Draw the Axis, Ticks, & Labels -->\n    <g id=\"axis-layer\">\n      <!-- Draw the axis -->\n      <polyline \n        points=\"\n            {{ layout.plot.xmin }} {{ layout.plot.ymin }} \n            {{ layout.plot.xmin }} {{ layout.plot.ymax }} \n            {{ layout.plot.xmax }} {{ layout.plot.ymax }}\" \n        stroke=\"{{ layout.color.axis }}\" \n        stroke-linecap=\"round\" \n        fill=\"transparent\" \n        />\n        \n      <!-- Draw the yaxis label (rotated) -->\n      <g transform=\"translate(0,20)\">\n            <text \n              x=\"30\" \n              y=\"176.5\" \n              fill=\"{{ layout.color.axis }}\" \n              font-family=\"helvetica\" \n              text-align=\"center\" \n              transform=\"rotate(-90,30,176.5)\"\n              >\n              {{ data.ylabel }}\n          </text>\n       </g>\n      \n      <!-- Draw the yticks -->\n      <g id=\"yticks\">\n      {% for t in yticks %}\n          <line \n              x1=\"{{ layout.plot.xmin }}\" \n              y1=\"{{ t.pos }}\" \n              x2=\"{{ layout.plot.xmin - layout.plot.pad }}\" \n              y2=\"{{ t.pos }}\" \n              stroke=\"{{ layout.color.axis }}\" \n              stroke-linecap=\"round\"\n              />\n          <text \n              x=\"{{ layout.plot.xmin }}\" \n              y=\"{{ t.pos }}\" \n              fill=\"{{ layout.color.axis }}\"\n              font-family=\"helvetica\"\n              text-anchor=\"end\"\n              dx=\"-15\"\n              dy=\"5\"\n              font-weight=\"lighter\"\n              >\n              {{ t.text }}\n          </text>\n      {%- endfor %}\n      </g>\n      \n      <!-- Draw the x axis label -->\n      <text \n          x=\"322.5\" \n          y=\"350\" \n          fill=\"{{ layout.color.axis }}\" \n          font-family=\"helvetica\" \n          text-anchor=\"middle\"\n          >\n          {{ data.xlabel }}\n      </text>\n      \n      <!-- Draw the xticks -->\n      <g id=\"xticks\">\n      {%- for t in xticks -%}\n          <line \n              x1=\"{{ t.pos }}\" \n              y1=\"{{ layout.plot.ymax }}\" \n              x2=\"{{ t.pos }}\" \n              y2=\"{{ layout.plot.ymax + layout.plot.pad }}\" \n              stroke=\"{{ layout.color.axis }}\" \n              stroke-linecap=\"round\"\n              />\n          <text \n              x=\"{{ t.pos }}\" \n              y=\"{{ layout.plot.ymax }}\" \n              fill=\"{{ layout.color.axis }}\"\n              font-family=\"helvetica\"\n              text-anchor=\"middle\"\n              dx=\"0\"\n              dy=\"25\"\n              font-weight=\"lighter\"\n              >\n              {{ t.text }}\n          </text>\n      {% endfor -%}\n      </g>\n    </g>\n    \n    <!-- Add the title, subtitle, & description -->\n    <g id=\"text-layer\">\n      <text \n          x=\"15\" \n          y=\"40\" \n          style=\"\n            font-family: helvetica; \n            font-size: 28px; \n            font-weight: normal;\n          \"\n          fill=\"{{ layout.color.title }}\"\n          >\n          {{ data.title }}\n      </text>\n      <text \n          x=\"15\" \n          y=\"70\" \n          style=\"\n            font-family: helvetica; \n            font-size: 18px; \n            font-weight: normal;\n          \"\n          fill=\"{{ layout.color.subtitle }}\">\n          {{ data.subtitle }}\n      </text>\n      <text \n          x=\"15\" \n          y=\"385\" \n          style=\"\n            font-family: helvetica; \n            font-size: 10px; \n            font-weight: normal;\n          \"\n          fill=\"{{ layout.color.caption }}\">\n          {{ data.caption }}\n      </text>\n   </g>\n</svg>\n```\n\nLet's assume our template is stored as a string in the variable `plot_template`.\n\nNow we can render the template, using our plot and layout data, with the following code:\n\n```py\nimport jinja2\n\n# Use the string to create a jinja template\njinja_template = jinja2.Template(\n    plot_template\n)\n\n# Render it by passing the data\n# to the template\nsvg_string = jinja_template.render(\n    data=data,\n    layout=layout,\n    xticks=xticks,\n    yticks=yticks,\n    xscale=xscale,\n    yscale=yscale\n)\n\n# The result is just a string\n# with the rendered plot\ntype(svg_string) == str\n\n\n# Now, optionally, if you want\n# to display the SVG in Jupyter\n# you can do this...\n\nfrom IPython.display import display_svg\n\ndisplay_svg(\n  svg_string,\n  raw=True\n)\n```\n\nHere's what the rendered plot would look like as an SVG:\n\n![svg](/images/jinja-sample-plot.svg)\n*Sample plot rendered with jinja (as an SVG)*\n\nAnd here's what it would look like as a raster image:\n\nFor the complete code, check out the gist [here](https://gist.github.com/a-poor/173f0d190634065bfd7ad73eb882e637).\n\n## Conclusion\n\nIf you're just doing EDA calling `plt.scatter(x,y)` is still the way to go, but if you want to be able to fully customize your plot try using Jinja. It might take more time to start up but your templates are easily reusable and you aren't limited to the plots that are supported by your plotting library.\n\nAnother great benefit of SVGs is the ability to copy the path from another SVG and incorporate it into your own. For example, if you wanted to change the points in your scatter plot to be your company's logo and your competitors, all you need to do is download the SVG logos and replace the circles in your template with the logo paths.\n\n",
    "collection": "blog",
    "data": {
      "isDraft": false,
      "title": "Take Full Control of Your Python Plots with Jinja",
      "subtitle": "Create fully custom plots in Python with SVG and Jinja",
      "description": "Next time you want to make a fully customized plot in Python ditch matplotlib and try Jinja. Here's a short tutorial to help you get started.",
      "image": {
        "src": "/images/jinja-data-crayon-dalle.webp",
        "alt": "A crayon drawing of a data visualization, generated by DALL-E.",
        "caption": "A crayon drawing of a data visualization, generated by DALL-E."
      },
      "tags": [
        "python",
        "jinja",
        "data-visualization",
        "data-science",
        "svg"
      ],
      "publishDate": "2020-07-02T00:00:00.000Z",
      "recommended": [
        "data-science-profilers",
        "flask-ml-predictions",
        "algorithmic-color-palettes"
      ]
    }
  },
  {
    "id": "predict-spotify-skips.md",
    "slug": "predict-spotify-skips",
    "body": "\n## Introduction\n\nI worked on a slightly simplified version of the *Spotify Sequential Skip Prediction Challenge* [1] as my 3rd project for the Metis Data Science Bootcamp.\n\nSpotify — as I'm sure you're aware — is a streaming service whose business model is centered around delivering song recommendations to users in order to keep them using their app.\n\n> Spotify has over 190 million active users interacting with over 40 million tracks.\n\nThe goal of the challenge is to predict the likelihood of a user skipping any given song during a listening session.\n\n## Methodology\n\n### The Data\n\nFor the competition, Spotify supplied two main sets of information. One table has information about user listening sessions. For example:\n\n- Session ID\n- Position in the session\n- Track ID\n- If the track was skipped or not\n- Other session metadata\n\nNote that the user sessions were all between 10 and 20 tracks long and didn't include any identifiable information about the users.\n\nThe second table has metadata about the tracks (corresponding to the `track_id` feature from the session table), such as:\n\n- Track duration\n- Track popularity in the US\n- Release year\n\nAs well as some extra features, generated by Spotify, to describe the songs, such as:\n\n- `acousticness`\n- `beat strength`\n- `bounciness`\n- `danceability`\n\nThe dataset also includes a set of 8 “acoustic vectors” for each track which are latent encodings that Spotify has generated for each track.\n\nFor licensing reasons, Spotify anonymize the track information, so there aren't any data on things like track name, artist, album, or genre.\n\nThe targets in the provided dataset — whether or not the track was skipped — are balanced with about 51.7% of the tracks being skipped. So there was no need to adjust the class balances for training.\n\nThere was a surplus of data so I worked with a subset of about 100k rows from the session table and the corresponding rows from the track table, which I loaded into a Postgres database hosted on an AWS EC2 instance. The database schema is shown below:\n\n![Schema for Spotify Skip Data](/images/spotify-skip-data-schema.webp)\n*Schema for Spotify Skip Data*\n\nTo get a sense of what a single session might look like, the following image shows a plot of a single user session as it relates to one of the features — “track loudness”, a Spotify descriptor of the song's characteristics, not “volume” in the traditional sense — where the color indicates which tracks were skipped and not skipped.\n\n![Single Session's Track-Skips vs Track-Loudness](/images/spotify-single-session-skips-vs-loudness.webp)\n*Single Session's Track-Skips vs Track-Loudness*\n\nWhile not meaningful by itself, this illustrates how each session an be plotted against the song attributes. In this example, the user only listened to two of the first 11 tracks, and then listened to all of the remaining 9 tracks, as the loudness value decreased.\n\nFurther, in order to get a better sense of the overall distribution track attributes, I've plotted the distribution of a small subset of the features and highlighted where three different songs fall in the distribution (using data from the Spotify API). The three songs, from three different genres, are:\n\n- Willie Nelson's \"Highwayman\" (Country)\n- Wu-Tang Clan's “Protect Ya Neck” (Hip-Hop/R&B)\n- John Coltrane's “Blue World” (Jazz)\n\nBelow are the attribute distribution plots for the attributes “energy”, “loudness”, “danceability”, and “acousticness”:\n\n![](/images/spotify-track-features-applied-to-example-songs.webp)\n*Distribution of a Subset of Track Features with 3 Songs Plotted for Reference*\n\n### Feature Engineering\n\nThis data is inherently sequential. The model's skip-prediction for any given song will be based on the songs that were skipped earlier in the user listening session. Therefore, in order for my model to account for that, I added a group of features that represent the previous tracks' audio features as well as whether or not those tracks were skipped.\n\n*Side-Note: I also tried an alternative approach where I added two sets of features corresponding to the average track feature information for previous tracks that were skipped and tracks that were not skipped, respectively, but found that it took longer to compute, added twice the number of features, and didn't improve model performance.*\n\n### Model Selection\n\nThe target metric is accuracy, per competition guidelines.\n\nI started by baselining with a Logistic Regression model but found that it had poor accuracy. So from there, I moved on to tree-based models which would be less interpretable but would be able to automatically handle complex feature interactions.\n\n## Results\n\n![ROC Curve Comparing Model Performance for Logistic Regression and LightGBM Models](/images/spotify-roc-logistic-regression-vs-lightgbm.webp)\n*ROC Curve Comparing Model Performance for Logistic Regression and LightGBM Models*\n\nMy best model's final test accuracy was **0.73**, using LightGBM's `LGBMClassifier` model, which is fairly good given the problem but with room for improvement.\n\nAfter training the model, I analyzed the errors, looking for areas to improve the model, but didn't find any clear trends in the residuals.\n\n## Conclusions\n\n![Final LightGBM Model's Relative Feature Importance Ranking](/images/spotify-lightgbm-feature-importance-graph.webp)\n*Final LightGBM Model's Relative Feature Importance Ranking*\n\nThis figure shows the final model's ranking of the top 10 features by relative importance. The model ranked track popularity as the most important feature, followed by the track duration, and then the previous track's popularity. The ranking seems to generally make sense and isn't picking up on anything too surprising, other than possibly the fact that track popularity is ranked much higher than the next most important feature. Interestingly, though, this graph (as well as the graph of the full ranking which can be seen below, in the appendix) shows that, in a few cases, the model seems to be comparing features for the current track and the previous track. Some examples include popularity, duration, and loudness.\n\nAgain, the model performance is reasonable, with an accuracy of 0.73 compared to a completely naive model (always predicting that the track will be skipped) that would have an accuracy of 0.51, but still with plenty of room for improvement.\n\n## Future Work\n\nMoving forward, a few areas for continued work include:\n\n- Add unsupervised learning to cluster songs based on track features and possibly generate “pseudo-genres”\n- Make predictions using a Recurrent Neural Network, that could more naturally handle sequential track information\n- Supplement the dataset with more data from Spotify's API (e.x. genre information)\n- Create a Flask app that uses D3 to visualize model predictions and allow the user to interactively explore the dataset\n\nThanks for reading and I'd love to hear your thoughts!\n\n## Appendix\n\n- Acousticness: Likelihood a track is acoustic\n- Danceability: Describes how suitable a track is for dancing\n- Energy: Measure of a track's intensity\n- Valence: Level of “positiveness” conveyed by a track\n- Speachiness: Detects the presence of spoken words\n\n### Feature Subset Description\n\n- *Acousticness*: Likelihood a track is acoustic\n- *Danceability*: Describes how suitable a track is for dancing\n- *Energy*: Measure of a track’s intensity\n- *Valence*: Level of “positiveness” conveyed by a track\n- *Speachiness*: Detects the presence of spoken words\n\n### Full Model Feature Importance Rank\n\n![Full Model Feature Importance Ranking](/images/spotify-full-model-feature-importance-graph.webp)\n\n### Track “Acoustic Vector” Correlation Matrix\n\n![Track's “Acoustic Vector” Feature Correlation Matrix](/images/spotify-acoustic-vector-correlation-matrix.webp)\n\n### Citations\n\n[Here](https://www.crowdai.org/challenges/spotify-sequential-skip-prediction-challenge)'s a link to the challenge site (which has more information on the dataset and the challenge rules).\n\n[1] B. Brost, R. Mehrotra, and T. Jehan, The Music Streaming Sessions Dataset (2019), Proceedings of the 2019 Web Conference\n\n",
    "collection": "blog",
    "data": {
      "isDraft": false,
      "title": "Predicting Spotify Track Skips",
      "subtitle": "Working on the Spotify Sequential Skip Prediction Challenge",
      "description": "Metis Data Science Bootcamp project using machine learning to predict Spotify user track skips for the \"Spotify Sequential Skip Prediction Challenge\".",
      "image": {
        "src": "/images/spotify-skip-cover.webp",
        "alt": "Screenshots from the Spotify desktop app.",
        "caption": "Screenshots from the Spotify desktop app"
      },
      "tags": [
        "spotify",
        "data-science",
        "python",
        "machine-learning",
        "bootcamp"
      ],
      "publishDate": "2020-02-18T00:00:00.000Z",
      "recommended": [
        "algorithmic-color-palettes",
        "big-query-data-augmentation",
        "serve-ml-with-grpc"
      ]
    }
  },
  {
    "id": "serve-ml-with-grpc.md",
    "slug": "serve-ml-with-grpc",
    "body": "\nMost people who are looking to put their newly trained ML model into production turn to REST¹ APIs. Here's why I think you should consider using gRPC instead.\n\n## Wait! What's wrong with REST!?\n\nNothing! The main benefit of REST APIs is their ubiquity. Every major programming language has a way of making HTTP clients and servers. And there are several existing frameworks for wrapping ML models with REST APIs (e.g. BentoML, TF Serving, etc). But, if your use case doesn't fit one of those tools (and even if it does), you may find yourself wanting to write something a little more custom. And the same thing that makes REST APIs versatile can also make them difficult to work with.\n\n## What is gRPC?\n\nAs its [site states](https://grpc.io/), gRPC is “a high performance, open source universal RPC framework,” originally developed at Google. The three main elements at the core of gRPC are: code generation, HTTP/2, and Protocol Buffers².\n\n[Protocol Buffers](https://developers.google.com/protocol-buffers) are a binary, structured data format designed by Google to be small and fast. Both gRPC services and their request/response message formats are defined in `.protobuf` files.\n\ngRPC client and server code is generated from the `.protobuf` definition files, in your preferred language. You then fill in the business logic to implement the API.\n\n[HTTP/2](https://developers.google.com/web/fundamentals/performance/http2)-based transport provides gRPC with [several key benefits](https://grpc.io/blog/grpc-load-balancing/#why-grpc):\n\n- A binary protocol\n- Bi-directional streaming\n- Header compression\n- Multiplexing several requests on the same connection\n\n_Okay, but what does that mean for me?_\n\n## Type Safety & Documentation\n\nSince gRPC APIs are defined via protobufs, they are inherently documented and type-safe. Conversely, REST APIs have no such guarantee you would need extra tooling like OpenAPI to define and document your service as well as a library to validate client requests.\n\n## Speed, Binary Data & Streaming\n\ngRPC takes full advantage of HTTP/2 and Protocol Buffers to make your API as fast as possible. gRPC messages are made up of efficiently packed binary data compared with REST's plain-text, JSON-encoded messages.\n\n[One commonly cited test](https://medium.com/@EmperorRXF/evaluating-performance-of-rest-vs-grpc-1b8bdf0b22da) shows gRPC to be roughly 7-10 times faster than REST. While the differences may be less visible with smaller requests, the inputs to ML models can often be large (e.g. large tables of data, images to be processed, or even video), where compression and binary formats shine.\n\nIn fact, since Protocol Buffers allow for binary data, a request could be a large table of data encoded with [Apache Arrow](https://arrow.apache.org/) or [Apache Parquet](https://parquet.apache.org/). And further, thanks to the capabilities of HTTP/2, large binary messages can broken up into chunks and streamed.\n\n## Downsides & Alternatives\n\ngRPC certainly isn't perfect. For example, here are some issues you might run into:\n\n- Slower initial development\n- Less commonly used\n- Messages aren't human-readable, which makes debugging more difficult\n- Client libraries need to be generated\n\nOther approaches exist that might fit better with your workflow. BentoML, TF Serving, and Ray Serve are some great options for serving ML models. Or, if you're looking for something a little more customizable, FastAPI and Flask are two great options that might be a better match.\n\nAlso, for a partial approach, you could also consider swapping out your message format from JSON to BSON or MessagePack.\n\n## Conclusion / TL;DR\n\ngRPC APIs are fast and easy to work with. They are inherently type-safe, they allow for bi-directional streaming messages (e.g. breaking large files into chunks), and they use a fast and efficient message format (Protocol Buffers).\n\nNext time you need to serve up an ML model via an API, consider using gRPC!\n\n## Further Reading\n\n- gRPC (gRPC Python library)\n- Protocol Buffers\n- HTTP/2\n\n## Footnotes\n\n[1] I know the term “RESTful” is maybe thrown around a bit too liberally — and applied to any HTTP-based API. In this article, I use the colloquial definition of a REST API.  \n[2] This is a bit of an oversimplification - gRPC is highly customizable. For example, you could use JSON instead of protobufs and HTTP/1 instead of HTTP/2. But...should you?\n",
    "collection": "blog",
    "data": {
      "isDraft": false,
      "title": "Serving ML Models with gRPC",
      "subtitle": "Skip REST and give gRPC a try",
      "description": "gRPC APIs are fast, efficient, and type-safe. Next time you need to create an ML prediction service, ditch REST and give gRPC a shot!",
      "image": {
        "src": "/images/serve-ml-grpc-dalle.webp",
        "alt": "A DALL-E generated image of a brain with wires coming out of it.",
        "caption": "An image of a brain with wires coming out of it, generated by DALL-E."
      },
      "tags": [
        "api",
        "grpc",
        "rest",
        "machine-learning",
        "python"
      ],
      "publishDate": "2021-12-01T00:00:00.000Z",
      "recommended": [
        "flask-ml-predictions",
        "data-science-profilers",
        "js-in-rs"
      ]
    }
  },
  {
    "id": "zero-trust-programming.md",
    "slug": "zero-trust-programming",
    "body": "\nOpen source software is vital in modern software engineering and yet we as developers are faced with the increasingly difficult task of tracking the security of our dependancies -- both direct and indirect -- all while maintaining velocity.\n\n\"Zero Trust\" is a security model based on the idea \"never trust, always verify.\" It mandates continuous authentication and validation for every user and device, irrespective of their position inside or outside the network. This is in contrast to traditional IT network security, based on a _castle-and-moat_ model, where no one outside the network is given access but everyone inside the network is trusted and given access by default.\n\nBy applying the ideas of Zero Trust to the code we write -- what I'm calling _Zero Trust Programming_ -- we can simplify the process of auditing 3rd party code, plug existing vulnerabilities caused by _ostrich security_ (i.e. _head-in-the-sand_) and _GitHub-star-based-audits_, and drastically cut down on application attack surfaces.\n\n\n### The Problem at Hand\n\n![An isomorphic castle and moat rendering](/images/mj-isometric-castle-and-moat.webp)\n_A Castle and Moat -- Generated by Midjourney_\n\nThe benefits of using open source software are clear. But with that comes a cost -- the cost of babysitting the security of your dependancies and transitive dependancies.\n\nImagine you're building an API using Node.js with an endpoint that greets your users with a friendly message. You were going to write the code to generate nice, personalized greetings until you found a package on npm, `\"justgreetings\"`, that does exactly that. So you install it and run it locally to see how well it works. You make a request and get your personalized greeting! \n\nUnfortunately, in the meantime, that function that you thought was just generating greetings was also: reading in your environment variables (including your `OPENAI_API_KEY`) and your AWS credential file, sending an HTTP request with that data out to the internet, and then installing a crypto miner in the background.\n\nYou could sandbox your application inside a Docker container or use Deno's permission system. Both of these allow you to limit the blast radius by making that _castle-and-moat_ smaller, but as soon as one part of your application needs access to a file (`~/.aws/credentials`) or an environment variable (`OPENAI_API_KEY`) everything has access. Why can't we be more granular with our permissions?\n\nAccording to the Snyk 2022 Open Source Security Report, the average application development project has 49 vulnerabilities and 80 direct dependencies ([source](https://snyk.io/reports/open-source-security)). How much time would it take to audit those direct and indirect dependencies and how often would you need to do that? And how much time is left over?\n\nAccording to the Linux Foundation, between 70 and 90% of any given piece of modern software solution is made up of open source software ([source](https://www.linuxfoundation.org/blog/blog/a-summary-of-census-ii-open-source-software-application-libraries-the-world-depends-on)). So, for every line of code you write there are between 2 and 9 times more lines of open source dependancy code to be vetted.\n\nThis may be possible for a large company who can dedicate a team to vet 3rd party code but I don't think it's *reasonable*. And where does that leave the rest of us? From the smaller organization to the fast-moving startup to the indie dev. Over 40% of organizations don't have high confidence in their open source software security (according to the same Snyk report). What if there was a better approach to help bridge that gap?\n\n\n### Enter Zero Trust Programming\n\n![A baby robot playing with blocks in a sandbox at the park. (Generated by Midjourney)](/images/baby-robot-sandbox.webp)\n_A Baby Robot Trapped in a Sandbox -- Generated by Midjourney_\n\n_Zero Trust Programming_ (ZTP for short) takes the idea of \"Zero Trust\" as it relates to network security and applies it to the code you write; this allows you to define granular permissions that can be applied to functions, types, imports, and other scopes as needed. The current system follows the same _castle-and-moat_ paradigm, where once 3rd party code gets imported it has the same trusted access as 1st party code. \n\nReturning to the example of the `justgreetings` npm package but this time with ZPT, the application could import the 3rd party code without granting it any special permissions and run the code knowing it wouldn't be able to access any special resources. Even though other places in your application may require file system access, network access, or environment variable access, those same permissions aren't automatically granted everywhere within the castle walls of your codebase.\n\nThe idea of ZTP doesn't need to stop with 3rd party code, with system resources, or at the package level. You could also define resources that expose their own permissions within your application code. For example, say you have a function for loading database credentials. You could define an associated permission being exposed (e.g. \"db_secret_read\") which could then be granted elsewhere in the codebase (e.g. grant access to the database client or the `list_students` function).\n\n![An xkcd comic where the school is calling a mom because her son's name includes SQL injection](/images/xkcd-exploits-of-a-mom.png)\n_[xckd comic \"Exploits of a Mom\"](https://xkcd.com/327)_\n\nAdding permissions around 1st party code and resources can help to mitigate another class of vulnerability, code injection (as well as possible non-malicious, buggy code). For example, the function `\"list_students\"` would only have \"read\" permissions on the database and would be prevented from making any changes even if someone tries searching for the student `\"Robert'; DROP TABLE Students; --\"`.\n\nFinally, in addition to blocking unauthorized access to secured resources within the application, ZTP would be able to generate an audit log that details what permission boundaries are crossed, when, and by whom.\n\n\n### Challenges and Tradeoffs\n\n![A robot art thief repelling down from the ceiling into an art gallery](/images/mj-robot-art-thief.webp)\n_A Robot Art Thief -- Generated by Midjourney_\n\nOf course in the ideal case ZTP would be available in your current language or framework, intuitive, and frictionless but that's easier said than done. Tools like Open Policy Agent and AWS IAM have proven that granular, configurable permission systems can easily become overwhelming. If ZTP is too complex, too hard to understand, or too hard to use, developers either won't use it or will just use a blanket `Allow:*`. That's a place where TypeScript sets a great example -- allowing for gradual and selective use of ZTP removes the all-or-nothing fatigue. Start by limiting the permissions on some imported 3rd party packages and build on that foundation.\n\nAnother concern is the occasional need to break the rules. ZTP needs to be flexible enough to allow developers to use the tool as they see fit. One possible way to handle this might take inspiration from Rust's `unsafe` blocks. There could be the ability to add a block or scope with elevated permissions, allowing for deliberate rule-bending, and something that a static analysis or an auditor could call out.\n\nWhat would the performace impact be for a permission system this extensive? Potentially severe but ultimately the answer would come down to implementation. Ideally a fair amount of the cost could be avoided by enforcing rules at build-time or compile-time. And, as cooperation with the runtime would likely be essential, if these rules could be built-in that could also help. At the end of the day, though, there would be at least some amount of slow-down which would have to be weighed against the security benefits. The front door of my house slows me down but I'm okay with that.\n\nOne area that I see as the most challenging to work-around would be subprocesses and the FFI. I can't think of a good workaround for vulnerabilities that stem from these types of code. One option, which Deno implements, is the ability to limit which commands can be run but obviously extending the same ZTP security would be difficult if not impossible outside of the application runtime itself. Another approach -- while not a true fix -- would be to _strongly_ encourage those external functions and libraries to be made available as WASM, which could allow for the same performance and versatility of other languages and tools while providing the same level of granular control to the application runtime. \n\n\n### Implementing Zero Trust Programming\n\n![An illustration of a robot passing through a metal detector](/images/mj-robot-metal-detector.webp)\n_A Robot Passing Through a Metal Detector -- Generated by Midjourney_\n\nSince Zero Trust Programming is purely conceptual, it isn't necessarily tied to one language or framework. It could be implemented in an existing language like Python or TypeScript or it could be part of a new language (`ztp-lang`?).\n\nWhile a whole new language might allow for a cleaner design it would make adoption difficult. Everything listed in the previous section about gradual adoption would go out the window. Instead, I think the JavaScript/TypeScript ecosystem could be a promising starting point. ZTP could have it's own JS runtime -- like Node, Bun, or Deno -- that implements these zero trust security features.\n\nI don't know what would work best in terms of syntax; maybe it could transpile to JavaScript/TypeScript, like `TSX`, or use a special decorator syntax. Or it could use configuration comments, like eslint.\n\nIt would also be great if ZTP could be \"backwards compatable\" and permissions could be added after-the-fact, similar to how the TypeScript ecosystem has `\"@types/\"` packages which provide type information for JavaScript packages.\n\nSpecific syntax aside, the key to making ZTP accessible would be developer tooling -- static analyzers, IDE integrations, debugger support, visualizations of permission trees (e.g. DAG visualizations or flame graphs) -- and there would be a lot of room for integration with existing security tools and frameworks like Snyk and OpenTelemetry (e.g. audit logs).\n\nAt the end of the day, though, the community would be most important, when it comes to planning, implementing, integrating, testing, and documenting.\n\n\n### Conclusion\n\nZero Trust Programming would allow for granular control of the ways in which application components and 3rd party code access sensitive resources both within the application itself and within the broader context of the host environment. Through ZTP, developers would have the tools required to add whatever level of security fits best with their usecase. This is all still conceptual and far from having a practical implementation but I think the idea is at least interesting, if not promising.\n\nThank you for reading! I would love to hear your thoughts on the idea of Zero Trust Programming. What do you like? What would you change? Do you think it's practical and could be implemented? What's missing? Let me know! You can reach me on [LinkedIn](https://linkedin.com/in/austinpoor), [Twitter](https://twitter.com/austin_poor), [Mastodon](https://mastodon.social/@austinpoor), or [BlueSky](https://bsky.app/profile/austinpoor.com)!\n\n\n### Links\n\n- [Linux Foundation on Open Source Software](https://www.linuxfoundation.org/blog/blog/a-summary-of-census-ii-open-source-software-application-libraries-the-world-depends-on)\n- [Snyk 2022 Open Source Security Report](https://snyk.io/reports/open-source-security/)\n- [The Deno Permission System](https://deno.com/manual@v1.33.4/basics/permissions)\n- [WASI Capabilities](https://github.com/bytecodealliance/wasmtime/blob/main/docs/WASI-capabilities.md)\n- [CrowdStrike: Zero Trust Security](https://www.crowdstrike.com/cybersecurity-101/zero-trust-security/)\n- [CloudFlare: Zero Trust](https://www.cloudflare.com/learning/security/glossary/what-is-zero-trust/)\n- [CloudFlare: Castle-and-Moat Security](https://www.cloudflare.com/learning/access-management/castle-and-moat-network-security/)\n- [WikiPedia: Zero Trust Security Model](https://en.wikipedia.org/wiki/Zero_trust_security_model)\n- [Unsafe Rust](https://doc.rust-lang.org/book/ch19-01-unsafe-rust.html)\n\n",
    "collection": "blog",
    "data": {
      "isDraft": false,
      "title": "On Zero Trust Programming",
      "subtitle": "Applying the Concept of Zero Trust Security to the Code We Write",
      "description": "Explore the concept of Zero Trust Programming (ZTP), which applies the Zero Trust security model to code, allowing granular permissions and enhancing open source software security. Discover ZTP's potential benefits in reducing vulnerabilities and improving audit processes, while considering the challenges and tradeoffs. Learn how ZTP could be implemented in existing or new languages and ecosystems.",
      "image": {
        "src": "/images/mj-robot-airport-security.webp",
        "alt": "An illustration of a robot passing through security. Generated by Midjourney.",
        "caption": "A Robot Walking Through Airport Security — Generated by Midjourney"
      },
      "tags": [
        "zero trust programming",
        "javascript",
        "open source software",
        "open source security",
        "software security"
      ],
      "publishDate": "2023-05-24T00:00:00.000Z",
      "recommended": [
        "js-in-rs",
        "apoor-dot-dev",
        "algorithmic-color-palettes"
      ]
    }
  }
]